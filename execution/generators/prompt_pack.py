"""
Prompt Pack Generator for the Product Factory.

Generates curated prompt pack products including:
- prompts.json: Structured prompt data
- prompts.md: Human-readable markdown version
- README.md: How to use the pack
- QUICK_START.md: 5 best prompts to try first
"""

import json
from typing import Optional

from .base_generator import BaseGenerator, GeneratedProduct, ProductSpec

# Structure for prompt packs
PROMPT_PACK_STRUCTURE = {
    "categories": "3-5 categories based on use case",
    "prompts_per_category": "5-10 prompts each",
    "prompt_fields": {
        "title": "Short, descriptive title",
        "prompt_text": "The actual prompt to copy/paste",
        "expected_output_description": "What the AI will produce",
        "example_output": "Sample of what to expect",
    },
}

# Prompt for Claude to generate the prompt pack
PROMPT_PACK_PROMPT = """You are creating a curated prompt pack for AI tools like ChatGPT, Claude, etc.

Given the following product specification:
- Problem being solved: {problem}
- Solution name: {solution_name}
- Target audience: {target_audience}

Generate a comprehensive prompt pack as JSON with the following structure:

{{
    "title": "<Pack title>",
    "description": "<What this pack helps users achieve, 1-2 sentences>",
    "categories": [
        {{
            "name": "<Category name>",
            "description": "<What prompts in this category do>",
            "prompts": [
                {{
                    "title": "<Prompt title, 5-10 words>",
                    "prompt_text": "<The actual prompt, ready to copy/paste. Include placeholders like [YOUR PRODUCT], [YOUR AUDIENCE], etc. Make it specific and actionable, at least 30 characters>",
                    "expected_output_description": "<What the AI will produce when using this prompt>",
                    "example_output": "<A brief example of the expected output, 2-3 sentences>"
                }}
            ]
        }}
    ]
}}

Requirements:
1. Create 3-5 categories covering different aspects of {problem}
2. Include 5-10 prompts per category (at least 15 total prompts)
3. Each prompt should be:
   - Specific and actionable (not vague)
   - Ready to copy/paste into any AI tool
   - Include placeholders like [YOUR PRODUCT], [YOUR INDUSTRY], [YOUR AUDIENCE] for customization
   - At least 30 characters long
4. Order prompts from most common use case to advanced
5. Return ONLY valid JSON, no markdown code blocks

Generate the prompt pack now:"""


# README template for prompt packs
README_TEMPLATE = """# {title}

{description}

## What's Included

This prompt pack contains **{total_prompts} ready-to-use prompts** organized into {num_categories} categories:

{category_list}

## How to Use

### 1. Choose Your AI Tool

These prompts work with:
- ChatGPT (GPT-4, GPT-3.5)
- Claude (Claude 3, Claude 2)
- Google Gemini
- Other AI assistants

### 2. Copy & Customize

1. Browse the prompts in `prompts.md` or `prompts.json`
2. Find a prompt that fits your need
3. Copy the prompt text
4. Replace placeholders (like `[YOUR PRODUCT]`) with your specific details
5. Paste into your AI tool

### 3. Iterate & Refine

- Use the AI's response as a starting point
- Ask follow-up questions for more detail
- Save responses that work well for future reference

## Quick Start

New to AI prompts? Start with `QUICK_START.md` - it has the 5 most impactful prompts to get you going immediately.

## File Reference

| File | Description |
|------|-------------|
| `prompts.json` | Structured data for programmatic access |
| `prompts.md` | Human-readable markdown format |
| `README.md` | This guide |
| `QUICK_START.md` | 5 best prompts to start with |

## Tips for Best Results

1. **Be Specific**: The more context you provide when filling placeholders, the better results you'll get
2. **Chain Prompts**: Use output from one prompt as input to another
3. **Save Good Outputs**: Keep a doc of responses that work well for you
4. **Experiment**: Try different phrasings if you don't get what you need

---

*Generated by Product Factory*
"""


# Quick start template
QUICK_START_TEMPLATE = """# Quick Start: 5 Best Prompts

Don't know where to begin? Start with these 5 high-impact prompts.

{prompts_content}

---

**Next Steps:**
- See `prompts.md` for all {total_prompts} prompts
- Read `README.md` for usage tips

*These 5 prompts were selected as the most immediately useful for {target_audience}.*
"""


class PromptPackGenerator(BaseGenerator):
    """
    Generator for curated prompt pack products.

    Creates prompt collections organized by category, with markdown formatting
    and quick start guides.
    """

    def __init__(self, claude_client=None):
        """
        Initialize the prompt pack generator.

        Args:
            claude_client: Optional Claude API client for AI-assisted generation.
        """
        super().__init__(claude_client)

    def get_product_type(self) -> str:
        """Return the product type for prompt packs."""
        return "prompt_pack"

    def generate(self, spec: ProductSpec) -> GeneratedProduct:
        """
        Generate a prompt pack product.

        Args:
            spec: ProductSpec with problem, solution_name, target_audience

        Returns:
            GeneratedProduct with prompts.json, prompts.md, README.md, QUICK_START.md
        """
        # Format the prompt with spec values
        prompt = PROMPT_PACK_PROMPT.format(
            problem=spec.problem,
            solution_name=spec.solution_name,
            target_audience=spec.target_audience,
        )

        # Generate prompt pack using Claude
        if self.claude_client:
            response = self.claude_client.generate(prompt)
            prompts_data = self._parse_prompts_data(response)
        else:
            # Fallback for testing without Claude client
            prompts_data = self._generate_fallback_pack(spec)

        # Create the files
        files = {}

        # 1. prompts.json - Structured prompt data
        files["prompts.json"] = json.dumps(prompts_data, indent=2).encode("utf-8")

        # 2. prompts.md - Human-readable markdown version
        prompts_md = self._format_prompts_markdown(prompts_data)
        files["prompts.md"] = prompts_md.encode("utf-8")

        # 3. README.md - How to use the pack
        readme = self._generate_readme(prompts_data, spec)
        files["README.md"] = readme.encode("utf-8")

        # 4. QUICK_START.md - 5 best prompts to try first
        quick_start = self._generate_quick_start(prompts_data, spec)
        files["QUICK_START.md"] = quick_start.encode("utf-8")

        # Create manifest
        manifest = self._create_manifest(spec, list(files.keys()))

        # Add prompt pack-specific metadata
        manifest["total_prompts"] = self._count_prompts(prompts_data)
        manifest["categories"] = len(prompts_data.get("categories", []))

        return GeneratedProduct(files=files, manifest=manifest)

    def validate(self, product: GeneratedProduct) -> bool:
        """
        Validate a generated prompt pack.

        Checks:
        - At least 3 categories
        - At least 15 total prompts
        - Each prompt has title and prompt_text
        - No prompt is under 20 chars

        Args:
            product: GeneratedProduct to validate

        Returns:
            True if valid, False otherwise
        """
        # Check required files exist
        required_files = ["prompts.json", "prompts.md", "README.md", "QUICK_START.md"]
        for filename in required_files:
            if filename not in product.files:
                return False

        # Parse prompts.json
        try:
            prompts_bytes = product.files["prompts.json"]
            prompts_data = json.loads(prompts_bytes.decode("utf-8"))
        except (json.JSONDecodeError, UnicodeDecodeError):
            return False

        # Check at least 3 categories
        categories = prompts_data.get("categories", [])
        if len(categories) < 3:
            return False

        # Count total prompts and validate each
        total_prompts = 0
        for category in categories:
            prompts = category.get("prompts", [])
            for prompt in prompts:
                # Check required fields
                if "title" not in prompt or "prompt_text" not in prompt:
                    return False

                # Check prompt_text length
                if len(prompt["prompt_text"]) < 20:
                    return False

                total_prompts += 1

        # Check at least 15 prompts
        if total_prompts < 15:
            return False

        return True

    def _format_prompts_markdown(self, prompts_data: dict) -> str:
        """
        Format prompts as a readable markdown document.

        Args:
            prompts_data: Parsed prompt pack data

        Returns:
            Markdown formatted prompts
        """
        title = prompts_data.get("title", "Prompt Pack")
        description = prompts_data.get("description", "")
        categories = prompts_data.get("categories", [])

        lines = [
            f"# {title}",
            "",
            description,
            "",
            "---",
            "",
        ]

        for i, category in enumerate(categories, 1):
            cat_name = category.get("name", f"Category {i}")
            cat_desc = category.get("description", "")
            prompts = category.get("prompts", [])

            lines.append(f"## {i}. {cat_name}")
            lines.append("")
            if cat_desc:
                lines.append(f"*{cat_desc}*")
                lines.append("")

            for j, prompt in enumerate(prompts, 1):
                prompt_title = prompt.get("title", f"Prompt {j}")
                prompt_text = prompt.get("prompt_text", "")
                expected = prompt.get("expected_output_description", "")
                example = prompt.get("example_output", "")

                lines.append(f"### {i}.{j} {prompt_title}")
                lines.append("")
                lines.append("**Prompt:**")
                lines.append("```")
                lines.append(prompt_text)
                lines.append("```")
                lines.append("")

                if expected:
                    lines.append(f"**What you'll get:** {expected}")
                    lines.append("")

                if example:
                    lines.append("**Example output:**")
                    lines.append(f"> {example}")
                    lines.append("")

                lines.append("---")
                lines.append("")

        return "\n".join(lines)

    def _generate_quick_start(self, prompts_data: dict, spec: ProductSpec) -> str:
        """
        Select 5 most impactful prompts for quick start guide.

        Args:
            prompts_data: Parsed prompt pack data
            spec: Original ProductSpec

        Returns:
            Formatted quick start markdown
        """
        categories = prompts_data.get("categories", [])

        # Collect first prompt from each category (most common use cases)
        # Then fill remaining slots with second prompts
        selected_prompts = []

        # First pass: get first prompt from each category
        for category in categories:
            prompts = category.get("prompts", [])
            if prompts:
                selected_prompts.append(
                    {
                        "category": category.get("name", ""),
                        "prompt": prompts[0],
                    }
                )
                if len(selected_prompts) >= 5:
                    break

        # Second pass: if we need more, get second prompts
        if len(selected_prompts) < 5:
            for category in categories:
                prompts = category.get("prompts", [])
                if len(prompts) > 1:
                    selected_prompts.append(
                        {
                            "category": category.get("name", ""),
                            "prompt": prompts[1],
                        }
                    )
                    if len(selected_prompts) >= 5:
                        break

        # Format selected prompts
        prompt_lines = []
        for i, item in enumerate(selected_prompts[:5], 1):
            prompt = item["prompt"]
            category = item["category"]

            prompt_lines.append(f"## {i}. {prompt.get('title', f'Prompt {i}')}")
            prompt_lines.append(f"*From: {category}*")
            prompt_lines.append("")
            prompt_lines.append("```")
            prompt_lines.append(prompt.get("prompt_text", ""))
            prompt_lines.append("```")
            prompt_lines.append("")
            if prompt.get("expected_output_description"):
                prompt_lines.append(
                    f"**What you'll get:** {prompt.get('expected_output_description')}"
                )
                prompt_lines.append("")

        prompts_content = "\n".join(prompt_lines)
        total_prompts = self._count_prompts(prompts_data)

        return QUICK_START_TEMPLATE.format(
            prompts_content=prompts_content,
            total_prompts=total_prompts,
            target_audience=spec.target_audience,
        )

    def _generate_readme(self, prompts_data: dict, spec: ProductSpec) -> str:
        """
        Generate the README file.

        Args:
            prompts_data: Parsed prompt pack data
            spec: Original ProductSpec

        Returns:
            Formatted README markdown
        """
        title = prompts_data.get("title", spec.solution_name)
        description = prompts_data.get(
            "description", f"Prompts to help with {spec.problem}"
        )
        categories = prompts_data.get("categories", [])
        total_prompts = self._count_prompts(prompts_data)

        # Build category list
        category_lines = []
        for category in categories:
            name = category.get("name", "Category")
            prompt_count = len(category.get("prompts", []))
            category_lines.append(f"- **{name}** ({prompt_count} prompts)")

        category_list = "\n".join(category_lines)

        return README_TEMPLATE.format(
            title=title,
            description=description,
            total_prompts=total_prompts,
            num_categories=len(categories),
            category_list=category_list,
        )

    def _count_prompts(self, prompts_data: dict) -> int:
        """
        Count total prompts in the pack.

        Args:
            prompts_data: Parsed prompt pack data

        Returns:
            Total number of prompts
        """
        total = 0
        for category in prompts_data.get("categories", []):
            total += len(category.get("prompts", []))
        return total

    def _parse_prompts_data(self, response: str) -> dict:
        """
        Parse Claude's response into prompt pack data.

        Args:
            response: Raw response from Claude

        Returns:
            Parsed prompt pack dict
        """
        try:
            # Handle potential markdown code blocks
            clean_response = response.strip()
            if clean_response.startswith("```"):
                lines = clean_response.split("\n")
                json_lines = []
                in_block = False
                for line in lines:
                    if line.startswith("```") and not in_block:
                        in_block = True
                        continue
                    elif line.startswith("```") and in_block:
                        break
                    elif in_block:
                        json_lines.append(line)
                clean_response = "\n".join(json_lines)

            return json.loads(clean_response)
        except json.JSONDecodeError:
            # Return minimal valid structure
            return {
                "title": "Prompt Pack",
                "description": "",
                "categories": [],
            }

    def _generate_fallback_pack(self, spec: ProductSpec) -> dict:
        """
        Generate a fallback prompt pack when Claude client is not available.

        Used for testing and development.

        Args:
            spec: ProductSpec to generate pack from

        Returns:
            Basic prompt pack dict
        """
        return {
            "title": f"{spec.solution_name} Prompt Pack",
            "description": f"A curated collection of prompts to help {spec.target_audience} with {spec.problem}.",
            "categories": [
                {
                    "name": "Getting Started",
                    "description": "Foundation prompts for beginners",
                    "prompts": [
                        {
                            "title": "Quick Assessment",
                            "prompt_text": f"I'm dealing with {spec.problem}. Analyze my current situation: [DESCRIBE YOUR SITUATION]. What are the top 3 priorities I should focus on first?",
                            "expected_output_description": "A prioritized list of 3 focus areas with reasoning",
                            "example_output": "Based on your situation, focus on: 1) Immediate issue, 2) Quick win, 3) Foundation building",
                        },
                        {
                            "title": "Action Plan Creator",
                            "prompt_text": f"Create a 30-day action plan for [YOUR SPECIFIC GOAL] related to {spec.problem}. Include weekly milestones and daily tasks.",
                            "expected_output_description": "A structured 30-day plan with clear milestones",
                            "example_output": "Week 1: Foundation... Week 2: Implementation... Week 3-4: Optimization...",
                        },
                        {
                            "title": "Best Practices Checklist",
                            "prompt_text": f"What are the essential best practices for {spec.target_audience} dealing with {spec.problem}? Give me a checklist I can use weekly.",
                            "expected_output_description": "A reusable checklist of best practices",
                            "example_output": "Weekly Checklist: [ ] Review metrics, [ ] Check processes, [ ] Update documentation...",
                        },
                        {
                            "title": "Common Mistakes Audit",
                            "prompt_text": f"What are the most common mistakes {spec.target_audience} make when dealing with {spec.problem}? How can I check if I'm making them?",
                            "expected_output_description": "List of common mistakes with self-audit questions",
                            "example_output": "Mistake 1: Not tracking metrics. Ask yourself: Do I know my key numbers?",
                        },
                        {
                            "title": "Resource Guide",
                            "prompt_text": f"What tools, resources, and skills should {spec.target_audience} have to effectively handle {spec.problem}?",
                            "expected_output_description": "A categorized list of recommended resources",
                            "example_output": "Essential tools: X, Y, Z. Skills to develop: A, B, C.",
                        },
                    ],
                },
                {
                    "name": "Problem Solving",
                    "description": "Prompts for tackling specific challenges",
                    "prompts": [
                        {
                            "title": "Root Cause Analysis",
                            "prompt_text": f"I'm experiencing [DESCRIBE PROBLEM] related to {spec.problem}. Help me identify the root cause using the 5 Whys technique.",
                            "expected_output_description": "A 5 Whys analysis leading to root cause",
                            "example_output": "Why 1: Sales are down because... Why 2: That's because...",
                        },
                        {
                            "title": "Solution Brainstorm",
                            "prompt_text": f"Generate 10 creative solutions for [SPECIFIC CHALLENGE] that {spec.target_audience} faces. Include both conventional and unconventional approaches.",
                            "expected_output_description": "10 solution ideas ranging from simple to creative",
                            "example_output": "1. Simple fix: X. 2. Process change: Y. 3. Creative approach: Z...",
                        },
                        {
                            "title": "Decision Matrix",
                            "prompt_text": f"Help me decide between [OPTION A] and [OPTION B] for addressing {spec.problem}. Create a pros/cons analysis with weighted scoring.",
                            "expected_output_description": "A decision matrix with weighted criteria",
                            "example_output": "Criteria 1 (weight 5): Option A scores 4, Option B scores 3...",
                        },
                        {
                            "title": "Risk Assessment",
                            "prompt_text": f"What are the risks of [PROPOSED ACTION] when dealing with {spec.problem}? Rate each risk by likelihood and impact.",
                            "expected_output_description": "Risk matrix with mitigation strategies",
                            "example_output": "Risk 1: High likelihood, Medium impact. Mitigation: ...",
                        },
                        {
                            "title": "Quick Fix Finder",
                            "prompt_text": f"I need a quick solution for [URGENT PROBLEM] related to {spec.problem}. What can I do in the next 24 hours to improve the situation?",
                            "expected_output_description": "Immediate action steps for quick results",
                            "example_output": "In the next 24 hours: 1) Stop doing X, 2) Start doing Y, 3) Communicate Z...",
                        },
                    ],
                },
                {
                    "name": "Strategy & Planning",
                    "description": "Long-term strategic thinking prompts",
                    "prompts": [
                        {
                            "title": "Strategic Framework",
                            "prompt_text": f"Create a strategic framework for {spec.target_audience} to address {spec.problem} over the next [TIME PERIOD]. Include goals, metrics, and milestones.",
                            "expected_output_description": "A comprehensive strategic framework",
                            "example_output": "Vision: X. Goals: A, B, C. Metrics: 1, 2, 3. Quarterly milestones...",
                        },
                        {
                            "title": "Competitive Analysis",
                            "prompt_text": f"How do successful {spec.target_audience} handle {spec.problem}? What can I learn from their approaches?",
                            "expected_output_description": "Analysis of successful approaches with takeaways",
                            "example_output": "Best performers do: X, Y, Z. Key insight: Focus on...",
                        },
                        {
                            "title": "Future Scenario Planning",
                            "prompt_text": f"What are 3 possible future scenarios for {spec.problem} in the next 2-3 years? How should I prepare for each?",
                            "expected_output_description": "Three scenarios with preparation strategies",
                            "example_output": "Scenario 1 (Optimistic): ... Preparation: ...",
                        },
                        {
                            "title": "Resource Allocation",
                            "prompt_text": f"How should I allocate my [TIME/BUDGET/RESOURCES] to most effectively address {spec.problem}? My constraints are [LIST CONSTRAINTS].",
                            "expected_output_description": "Optimized resource allocation plan",
                            "example_output": "Allocate 40% to high-impact area X, 30% to essential Y, 30% to growth Z...",
                        },
                        {
                            "title": "Success Metrics Definition",
                            "prompt_text": f"What KPIs and metrics should I track to measure my progress in addressing {spec.problem}? Include leading and lagging indicators.",
                            "expected_output_description": "A metrics framework with tracking cadence",
                            "example_output": "Leading indicators: X, Y. Lagging indicators: A, B. Track weekly: X, Monthly: A...",
                        },
                    ],
                },
            ],
        }
