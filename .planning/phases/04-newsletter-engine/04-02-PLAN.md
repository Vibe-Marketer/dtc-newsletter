---
phase: 04-newsletter-engine
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - execution/content_selector.py
  - execution/section_generators.py
  - tests/test_content_selector.py
  - tests/test_section_generators.py
autonomous: true

must_haves:
  truths:
    - "Content selector picks best content by outlier score"
    - "Different sources used when possible (diversity constraint)"
    - "Section 1 generates 30-60 word instant reward"
    - "Section 2 generates 300-500 word tactical content"
  artifacts:
    - path: "execution/content_selector.py"
      provides: "Content selection logic for each section"
      exports: ["select_content_for_sections", "ContentSelection"]
    - path: "execution/section_generators.py"
      provides: "Section 1 and 2 generators"
      exports: ["generate_section_1", "generate_section_2"]
  key_links:
    - from: "execution/content_selector.py"
      to: "content_aggregate output"
      via: "processes aggregated content items"
      pattern: "outlier_score"
    - from: "execution/section_generators.py"
      to: "execution/claude_client.py"
      via: "uses generate_section"
      pattern: "from.*claude_client.*import"
---

<objective>
Create content selection logic and first two section generators (Instant Reward + What's Working Now).

Purpose: Content selection ensures diverse, high-quality sources. Section 1 hooks readers immediately. Section 2 is THE MEAT - the most important section.
Output: Content selector module and section 1 & 2 generators with tests.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/04-newsletter-engine/04-CONTEXT.md
@.planning/phases/04-newsletter-engine/04-RESEARCH.md
@.planning/phases/04-newsletter-engine/04-01-SUMMARY.md
@execution/content_aggregate.py (content structure)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Content Selector Module</name>
  <files>
    execution/content_selector.py
    tests/test_content_selector.py
  </files>
  <action>
Create execution/content_selector.py with:

ContentSelection dataclass:
- section_1: dict | None  # Best quotable content
- section_2: dict | None  # Best tactical content  
- section_3: dict | None  # Best narrative content
- section_4_tool: str | None  # Tool recommendation (manual input for now)
- sources_used: list[str]  # Track source diversity

select_content_for_sections(aggregated: list[dict]) -> ContentSelection:
- Sort by outlier_score descending
- Apply diversity constraint: require at least 2 different sources in selections
- Selection logic per section:
  - Section 1: _is_quotable() - short title, has stats/numbers, or viral quote
  - Section 2: _is_tactical() - "how to", "step", "strategy", "tactic", "tip", "hack"
  - Section 3: _has_narrative_potential() - "case study", "story", "journey", "learned", "mistake"
- Track used content IDs to avoid exact duplicates
- Allow same source for different sections if content is transformed

Helper functions:
- _is_quotable(item: dict) -> bool: Check for quote-worthy content
- _is_tactical(item: dict) -> bool: Check for tactical how-to content
- _has_narrative_potential(item: dict) -> bool: Check for story potential
- _get_unique_sources(selection: ContentSelection) -> int: Count distinct sources

Handle sparse content:
- If no tactical content found, use highest outlier score item
- If no narrative content found, reuse tactical with different angle flag
- Return ContentSelection with None for unfillable sections

Tests should cover:
- Highest outlier score items selected
- Diversity constraint enforced (2+ sources)
- Tactical content detection (various keywords)
- Narrative content detection
- Quotable content detection (short titles, numbers)
- Sparse content fallback behavior
- Empty input handling
  </action>
  <verify>
Run `python -m pytest tests/test_content_selector.py -v`
All tests pass. Diversity constraint working.
  </verify>
  <done>
Content selector prioritizes by outlier score.
At least 2 different sources required.
Fallback logic handles sparse content days.
  </done>
</task>

<task type="auto">
  <name>Task 2: Section 1 and 2 Generators</name>
  <files>
    execution/section_generators.py
    tests/test_section_generators.py
  </files>
  <action>
Create execution/section_generators.py with:

generate_section_1(content: dict, client: ClaudeClient) -> str:
- Generates "Instant Reward" section (30-60 words)
- Content types: viral quote, striking stat, or engaging tweet
- Uses XML-structured prompt:
  ```
  <task>Generate Section 1: Instant Reward</task>
  <requirements>30-60 words, immediate value hook</requirements>
  <source_content>{content}</source_content>
  <output_format>Write directly, no headers</output_format>
  ```
- Returns generated text
- Validates word count (warn if outside 30-60 range, don't fail)

generate_section_2(content: dict, client: ClaudeClient, prior_sections: dict = None) -> str:
- Generates "What's Working Now" section (300-500 words)
- THE MEAT: single actionable tactic, not bullet lists
- Structure: Problem -> Solution -> How-to
- Uses prior_sections for context awareness
- XML-structured prompt:
  ```
  <task>Generate Section 2: What's Working Now</task>
  <requirements>
  - 300-500 words
  - Single actionable tactic (NOT bullet point lists)
  - Problem -> Solution -> How-to structure
  - Easy to implement, direct impact
  - Case study ONLY for incredibly valuable lessons
  </requirements>
  <source_content>{content}</source_content>
  <prior_context>{prior_sections summary if any}</prior_context>
  <output_format>Paragraphs only, no headers, no bullets</output_format>
  ```
- Returns generated text
- Validates word count (warn if outside 300-500 range)

Shared helper:
- _count_words(text: str) -> int: Word count utility
- _build_section_prompt(section_name: str, content: dict, extra_context: str = "") -> str

Tests should mock ClaudeClient:
- Section 1 generates with correct prompt structure
- Section 1 validates word count
- Section 2 generates with correct prompt structure
- Section 2 includes prior_sections in context
- Section 2 validates word count
- Empty content handling
  </action>
  <verify>
Run `python -m pytest tests/test_section_generators.py -v`
All tests pass. Prompt structures verified.
  </verify>
  <done>
Section 1 generator creates 30-60 word instant rewards.
Section 2 generator creates 300-500 word tactical content.
Prior sections passed for narrative coherence.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_content_selector.py tests/test_section_generators.py -v` - All tests pass
2. `grep -r "outlier_score" execution/content_selector.py` - Shows sorting by score
3. `grep -r "prior_sections" execution/section_generators.py` - Shows context passing
4. Check prompt templates include XML tags
</verification>

<success_criteria>
- Content selector picks by outlier score with diversity constraint
- Section 1 generates 30-60 word hook content
- Section 2 generates 300-500 word tactical content
- Prior sections passed for context
- All tests pass with mocked client
</success_criteria>

<output>
After completion, create `.planning/phases/04-newsletter-engine/04-02-SUMMARY.md`
</output>
