---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - execution/reddit_fetcher.py
  - execution/storage.py
  - tests/test_reddit_fetcher.py
  - tests/test_storage.py
autonomous: false
user_setup:
  - service: reddit
    why: "Reddit API access for content aggregation"
    env_vars:
      - name: REDDIT_CLIENT_ID
        source: "https://www.reddit.com/prefs/apps -> Create App -> script type"
      - name: REDDIT_CLIENT_SECRET
        source: "Same page, after creating app"
      - name: REDDIT_USER_AGENT
        source: "Format: dtc-newsletter-bot/1.0 by /u/yourusername"

must_haves:
  truths:
    - "Script can connect to Reddit API with valid credentials"
    - "Posts from r/shopify, r/dropship, r/ecommerce are fetched"
    - "Each post includes upvotes, title, selftext, created_utc"
    - "Subreddit average upvotes is calculated from recent posts"
    - "Posts are saved to data/content_cache/reddit/ as JSON"
    - "JSON includes full metadata: outlier_score, source, engagement_modifiers"
  artifacts:
    - path: "execution/reddit_fetcher.py"
      provides: "Reddit API integration"
      exports: ["fetch_subreddit_posts", "get_subreddit_average"]
    - path: "execution/storage.py"
      provides: "Content storage layer"
      exports: ["save_reddit_posts", "load_cached_posts"]
    - path: "data/content_cache/reddit/"
      provides: "Cached Reddit content"
  key_links:
    - from: "execution/reddit_fetcher.py"
      to: "praw"
      via: "PRAW Reddit instance"
      pattern: "praw\\.Reddit"
    - from: "execution/storage.py"
      to: "data/content_cache/reddit/"
      via: "JSON file writes"
      pattern: "json\\.dump"
    - from: "execution/reddit_fetcher.py"
      to: "execution/scoring.py"
      via: "import scoring functions"
      pattern: "from execution.scoring import"
---

<objective>
Implement Reddit content fetching and JSON storage.

Purpose: Connect to Reddit API via PRAW, fetch posts from target subreddits (r/shopify, r/dropship, r/ecommerce), calculate outlier scores using the scoring module, and save to JSON.

Output:
- execution/reddit_fetcher.py that fetches and scores posts
- execution/storage.py that persists posts to JSON files
- data/content_cache/reddit/ with actual fetched content
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/research/STACK.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Reddit Fetcher Module</name>
  <files>execution/reddit_fetcher.py, tests/test_reddit_fetcher.py</files>
  <action>
Create execution/reddit_fetcher.py:

```python
"""
Reddit content fetcher using PRAW.

Fetches posts from target subreddits and calculates outlier scores.
"""
import os
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import praw
from dotenv import load_dotenv

from execution.scoring import calculate_outlier_score

# Load environment variables
load_dotenv()

# Target subreddits for e-commerce content
TARGET_SUBREDDITS = ["shopify", "dropship", "ecommerce"]


def get_reddit_client() -> praw.Reddit:
    """
    Create authenticated Reddit client from environment variables.
    
    Required env vars:
    - REDDIT_CLIENT_ID
    - REDDIT_CLIENT_SECRET  
    - REDDIT_USER_AGENT
    
    Returns:
        Authenticated praw.Reddit instance
    
    Raises:
        ValueError: If required env vars are missing
    """
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT")
    
    if not all([client_id, client_secret, user_agent]):
        raise ValueError(
            "Missing Reddit credentials. Set REDDIT_CLIENT_ID, "
            "REDDIT_CLIENT_SECRET, and REDDIT_USER_AGENT in .env"
        )
    
    return praw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent
    )


def get_subreddit_average(reddit: praw.Reddit, subreddit_name: str, sample_size: int = 100) -> float:
    """
    Calculate average upvotes for a subreddit based on recent posts.
    
    Args:
        reddit: Authenticated Reddit client
        subreddit_name: Name of subreddit (without r/)
        sample_size: Number of posts to sample for average
    
    Returns:
        Average upvote count as float
    """
    subreddit = reddit.subreddit(subreddit_name)
    posts = list(subreddit.hot(limit=sample_size))
    
    if not posts:
        return 1.0  # Avoid division by zero
    
    total_upvotes = sum(post.score for post in posts)
    return total_upvotes / len(posts)


def fetch_subreddit_posts(
    reddit: praw.Reddit,
    subreddit_name: str,
    limit: int = 50,
    time_filter: str = "week"
) -> List[Dict[str, Any]]:
    """
    Fetch top posts from a subreddit with outlier scores.
    
    Args:
        reddit: Authenticated Reddit client
        subreddit_name: Name of subreddit (without r/)
        limit: Maximum posts to fetch
        time_filter: Time filter for top posts (hour, day, week, month, year, all)
    
    Returns:
        List of post dicts with outlier scores
    """
    subreddit = reddit.subreddit(subreddit_name)
    subreddit_avg = get_subreddit_average(reddit, subreddit_name)
    
    posts = []
    for post in subreddit.top(time_filter=time_filter, limit=limit):
        outlier_score = calculate_outlier_score(
            upvotes=post.score,
            subreddit_avg_upvotes=subreddit_avg,
            post_timestamp=post.created_utc,
            title=post.title,
            selftext=post.selftext or ""
        )
        
        posts.append({
            "id": post.id,
            "title": post.title,
            "selftext": post.selftext or "",
            "url": f"https://reddit.com{post.permalink}",
            "subreddit": subreddit_name,
            "upvotes": post.score,
            "num_comments": post.num_comments,
            "created_utc": post.created_utc,
            "author": str(post.author) if post.author else "[deleted]",
            "subreddit_avg_upvotes": subreddit_avg,
            "outlier_score": round(outlier_score, 2),
            "fetched_at": datetime.now(timezone.utc).isoformat()
        })
    
    return posts


def fetch_all_subreddits(
    subreddits: Optional[List[str]] = None,
    limit_per_sub: int = 50,
    min_outlier_score: float = 0.0
) -> List[Dict[str, Any]]:
    """
    Fetch posts from all target subreddits.
    
    Args:
        subreddits: List of subreddit names (defaults to TARGET_SUBREDDITS)
        limit_per_sub: Max posts per subreddit
        min_outlier_score: Filter to posts with score >= this value
    
    Returns:
        List of all posts, sorted by outlier score descending
    """
    if subreddits is None:
        subreddits = TARGET_SUBREDDITS
    
    reddit = get_reddit_client()
    all_posts = []
    
    for sub in subreddits:
        try:
            posts = fetch_subreddit_posts(reddit, sub, limit=limit_per_sub)
            all_posts.extend(posts)
        except Exception as e:
            print(f"Warning: Failed to fetch from r/{sub}: {e}")
            continue
    
    # Filter by minimum score
    if min_outlier_score > 0:
        all_posts = [p for p in all_posts if p["outlier_score"] >= min_outlier_score]
    
    # Sort by outlier score descending
    all_posts.sort(key=lambda x: x["outlier_score"], reverse=True)
    
    return all_posts
```

Create tests/test_reddit_fetcher.py:

```python
"""Tests for Reddit fetcher module."""
import pytest
from unittest.mock import Mock, patch, MagicMock
from execution.reddit_fetcher import (
    get_reddit_client,
    get_subreddit_average,
    fetch_subreddit_posts
)


class TestGetRedditClient:
    def test_raises_without_credentials(self):
        """Should raise ValueError when credentials missing."""
        with patch.dict('os.environ', {}, clear=True):
            with pytest.raises(ValueError, match="Missing Reddit credentials"):
                get_reddit_client()
    
    @patch('execution.reddit_fetcher.praw.Reddit')
    def test_creates_client_with_credentials(self, mock_reddit):
        """Should create Reddit client when credentials present."""
        with patch.dict('os.environ', {
            'REDDIT_CLIENT_ID': 'test_id',
            'REDDIT_CLIENT_SECRET': 'test_secret',
            'REDDIT_USER_AGENT': 'test_agent'
        }):
            client = get_reddit_client()
            mock_reddit.assert_called_once_with(
                client_id='test_id',
                client_secret='test_secret',
                user_agent='test_agent'
            )


class TestGetSubredditAverage:
    def test_calculates_average_correctly(self):
        """Should calculate average from post scores."""
        mock_reddit = Mock()
        mock_subreddit = Mock()
        mock_reddit.subreddit.return_value = mock_subreddit
        
        # Create mock posts with scores 100, 200, 300 (avg = 200)
        mock_posts = [Mock(score=100), Mock(score=200), Mock(score=300)]
        mock_subreddit.hot.return_value = mock_posts
        
        avg = get_subreddit_average(mock_reddit, "test_sub", sample_size=3)
        assert avg == 200.0
    
    def test_returns_one_for_empty_subreddit(self):
        """Should return 1.0 if no posts (avoid division by zero)."""
        mock_reddit = Mock()
        mock_subreddit = Mock()
        mock_reddit.subreddit.return_value = mock_subreddit
        mock_subreddit.hot.return_value = []
        
        avg = get_subreddit_average(mock_reddit, "empty_sub")
        assert avg == 1.0


class TestFetchSubredditPosts:
    def test_fetches_and_scores_posts(self):
        """Should fetch posts and calculate outlier scores."""
        mock_reddit = Mock()
        mock_subreddit = Mock()
        mock_reddit.subreddit.return_value = mock_subreddit
        
        # Mock hot posts for average calculation
        mock_subreddit.hot.return_value = [Mock(score=100)]
        
        # Create mock post for top fetch
        mock_post = Mock(
            id="abc123",
            title="Test post about making $5k",
            selftext="Content here",
            permalink="/r/test/comments/abc123/test",
            score=500,
            num_comments=50,
            created_utc=1700000000.0,
            author=Mock(__str__=lambda s: "testuser")
        )
        mock_subreddit.top.return_value = [mock_post]
        
        posts = fetch_subreddit_posts(mock_reddit, "test_sub", limit=1)
        
        assert len(posts) == 1
        assert posts[0]["id"] == "abc123"
        assert posts[0]["upvotes"] == 500
        assert posts[0]["outlier_score"] > 0
        assert "reddit.com" in posts[0]["url"]
```

Run tests to verify module works with mocked Reddit client.
  </action>
  <verify>
```bash
cd /Users/Naegele/dev/dtc-newsletter && python -m pytest tests/test_reddit_fetcher.py -v
```
  </verify>
  <done>
- get_reddit_client() creates authenticated PRAW client
- get_subreddit_average() calculates mean upvotes from recent posts
- fetch_subreddit_posts() returns posts with outlier scores
- Tests pass with mocked Reddit API
  </done>
</task>

<task type="auto">
  <name>Task 2: Storage Module</name>
  <files>execution/storage.py, tests/test_storage.py, data/content_cache/reddit/.gitkeep</files>
  <action>
Create execution/storage.py:

```python
"""
Content storage layer for caching aggregated content.

Stores posts as JSON files in data/content_cache/{source}/.
"""
import json
import os
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Dict, Any, Optional

# Base path for content cache
CACHE_BASE = Path("data/content_cache")


def ensure_cache_dir(source: str) -> Path:
    """
    Ensure cache directory exists for a source.
    
    Args:
        source: Source name (e.g., "reddit", "youtube")
    
    Returns:
        Path to the cache directory
    """
    cache_dir = CACHE_BASE / source
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def save_reddit_posts(
    posts: List[Dict[str, Any]],
    filename: Optional[str] = None
) -> str:
    """
    Save Reddit posts to JSON file.
    
    Args:
        posts: List of post dicts from reddit_fetcher
        filename: Optional filename (defaults to timestamp-based)
    
    Returns:
        Path to saved file
    """
    cache_dir = ensure_cache_dir("reddit")
    
    if filename is None:
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
        filename = f"reddit_{timestamp}.json"
    
    filepath = cache_dir / filename
    
    # Add metadata wrapper
    data = {
        "source": "reddit",
        "fetched_at": datetime.now(timezone.utc).isoformat(),
        "post_count": len(posts),
        "posts": posts
    }
    
    with open(filepath, "w") as f:
        json.dump(data, f, indent=2)
    
    return str(filepath)


def load_cached_posts(
    source: str,
    filename: Optional[str] = None,
    min_outlier_score: float = 0.0
) -> List[Dict[str, Any]]:
    """
    Load cached posts from JSON file.
    
    Args:
        source: Source name (e.g., "reddit")
        filename: Specific file to load (or latest if None)
        min_outlier_score: Filter to posts >= this score
    
    Returns:
        List of post dicts
    """
    cache_dir = CACHE_BASE / source
    
    if not cache_dir.exists():
        return []
    
    if filename:
        filepath = cache_dir / filename
    else:
        # Get most recent file
        files = sorted(cache_dir.glob("*.json"), reverse=True)
        if not files:
            return []
        filepath = files[0]
    
    with open(filepath, "r") as f:
        data = json.load(f)
    
    posts = data.get("posts", [])
    
    # Filter by score
    if min_outlier_score > 0:
        posts = [p for p in posts if p.get("outlier_score", 0) >= min_outlier_score]
    
    return posts


def get_high_outlier_posts(source: str, min_score: float = 3.0) -> List[Dict[str, Any]]:
    """
    Get posts with outlier score >= min_score (default 3x average).
    
    Convenience function for filtering to high-performing content.
    """
    return load_cached_posts(source, min_outlier_score=min_score)
```

Create tests/test_storage.py:

```python
"""Tests for storage module."""
import pytest
import json
import tempfile
from pathlib import Path
from unittest.mock import patch
from execution.storage import (
    ensure_cache_dir,
    save_reddit_posts,
    load_cached_posts,
    get_high_outlier_posts
)


@pytest.fixture
def temp_cache_dir(tmp_path):
    """Create a temporary cache directory."""
    with patch('execution.storage.CACHE_BASE', tmp_path):
        yield tmp_path


class TestEnsureCacheDir:
    def test_creates_directory(self, temp_cache_dir):
        """Should create cache directory if it doesn't exist."""
        cache_path = ensure_cache_dir("reddit")
        assert cache_path.exists()
        assert cache_path.name == "reddit"


class TestSaveRedditPosts:
    def test_saves_posts_to_json(self, temp_cache_dir):
        """Should save posts with metadata wrapper."""
        posts = [
            {"id": "abc", "title": "Test", "outlier_score": 5.5},
            {"id": "def", "title": "Test 2", "outlier_score": 3.2}
        ]
        
        filepath = save_reddit_posts(posts, "test.json")
        
        with open(filepath) as f:
            data = json.load(f)
        
        assert data["source"] == "reddit"
        assert data["post_count"] == 2
        assert len(data["posts"]) == 2
        assert data["posts"][0]["outlier_score"] == 5.5
    
    def test_generates_timestamp_filename(self, temp_cache_dir):
        """Should generate timestamp-based filename if not provided."""
        filepath = save_reddit_posts([{"id": "test"}])
        assert "reddit_" in filepath
        assert ".json" in filepath


class TestLoadCachedPosts:
    def test_loads_saved_posts(self, temp_cache_dir):
        """Should load posts from saved JSON."""
        posts = [
            {"id": "abc", "outlier_score": 5.0},
            {"id": "def", "outlier_score": 2.0}
        ]
        save_reddit_posts(posts, "test.json")
        
        loaded = load_cached_posts("reddit", "test.json")
        assert len(loaded) == 2
        assert loaded[0]["id"] == "abc"
    
    def test_filters_by_min_score(self, temp_cache_dir):
        """Should filter posts by minimum outlier score."""
        posts = [
            {"id": "high", "outlier_score": 5.0},
            {"id": "low", "outlier_score": 2.0}
        ]
        save_reddit_posts(posts, "test.json")
        
        loaded = load_cached_posts("reddit", "test.json", min_outlier_score=3.0)
        assert len(loaded) == 1
        assert loaded[0]["id"] == "high"
    
    def test_returns_empty_for_missing_source(self, temp_cache_dir):
        """Should return empty list if source directory doesn't exist."""
        loaded = load_cached_posts("nonexistent")
        assert loaded == []


class TestGetHighOutlierPosts:
    def test_returns_3x_outliers(self, temp_cache_dir):
        """Should return posts with score >= 3.0 by default."""
        posts = [
            {"id": "viral", "outlier_score": 6.0},
            {"id": "good", "outlier_score": 3.5},
            {"id": "meh", "outlier_score": 2.9}
        ]
        save_reddit_posts(posts, "test.json")
        
        high = get_high_outlier_posts("reddit")
        assert len(high) == 2
        assert all(p["outlier_score"] >= 3.0 for p in high)
```

Create data/content_cache/reddit/.gitkeep to ensure directory is tracked:
```bash
mkdir -p data/content_cache/reddit
touch data/content_cache/reddit/.gitkeep
```
  </action>
  <verify>
```bash
cd /Users/Naegele/dev/dtc-newsletter && python -m pytest tests/test_storage.py -v
```
  </verify>
  <done>
- save_reddit_posts() writes JSON with metadata wrapper
- load_cached_posts() reads and filters by outlier score
- get_high_outlier_posts() returns only 3x+ outliers
- data/content_cache/reddit/ directory exists
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Reddit fetcher and storage modules with full test coverage</what-built>
  <how-to-verify>
1. Ensure you have Reddit API credentials configured:
   - Go to https://www.reddit.com/prefs/apps
   - Click "Create App" or "Create Another App"
   - Select "script" type
   - Set redirect URI to http://localhost:8080
   - Copy client ID (under app name) and client secret
   
2. Add credentials to .env file (copy from .env.example):
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

3. Test the live Reddit connection:
   ```bash
   cd /Users/Naegele/dev/dtc-newsletter
   python -c "
   from execution.reddit_fetcher import fetch_all_subreddits
   posts = fetch_all_subreddits(limit_per_sub=5, min_outlier_score=0)
   print(f'Fetched {len(posts)} posts')
   for p in posts[:3]:
       print(f'  - [{p[\"outlier_score\"]:.1f}x] {p[\"title\"][:50]}...')
   "
   ```

4. Verify posts are saved:
   ```bash
   ls -la data/content_cache/reddit/
   ```

Expected: See fetched posts with outlier scores printed, JSON file created in cache directory.
  </how-to-verify>
  <resume-signal>Type "approved" if Reddit API works and posts are being fetched/scored correctly, or describe issues encountered</resume-signal>
</task>

</tasks>

<verification>
```bash
# All tests pass
python -m pytest tests/test_reddit_fetcher.py tests/test_storage.py -v

# Modules importable
python -c "from execution.reddit_fetcher import fetch_all_subreddits; print('Fetcher OK')"
python -c "from execution.storage import save_reddit_posts; print('Storage OK')"

# Cache directory exists
ls data/content_cache/reddit/
```
</verification>

<success_criteria>
1. execution/reddit_fetcher.py exports fetch_all_subreddits, get_reddit_client
2. execution/storage.py exports save_reddit_posts, load_cached_posts, get_high_outlier_posts
3. data/content_cache/reddit/ directory exists
4. All unit tests pass
5. Live API test fetches real posts with outlier scores
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
