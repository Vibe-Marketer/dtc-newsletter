---
phase: 01-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - directives/content_aggregate.md
  - execution/content_aggregate.py
autonomous: true

must_haves:
  truths:
    - "Single command runs full Reddit aggregation pipeline"
    - "Output shows posts sorted by outlier score"
    - "Can filter to show only 3x+ outliers"
    - "Directive and script have matching DOE versions"
    - "AI summary placeholder exists for each post"
  artifacts:
    - path: "directives/content_aggregate.md"
      provides: "DOE directive for content aggregation"
      contains: "DOE-VERSION: 2026.01.29"
    - path: "execution/content_aggregate.py"
      provides: "Main aggregation script"
      contains: "DOE-VERSION: 2026.01.29"
  key_links:
    - from: "execution/content_aggregate.py"
      to: "execution/reddit_fetcher.py"
      via: "import fetch_all_subreddits"
      pattern: "from execution.reddit_fetcher import"
    - from: "execution/content_aggregate.py"
      to: "execution/storage.py"
      via: "import save_reddit_posts"
      pattern: "from execution.storage import"
    - from: "directives/content_aggregate.md"
      to: "execution/content_aggregate.py"
      via: "DOE pairing"
      pattern: "execution/content_aggregate.py"
---

<objective>
Create the DOE-paired directive and script for content aggregation.

Purpose: Crystallize the working Reddit pipeline into a DOE pattern (directive + script with matching versions) that can be executed with a single command.

Output:
- directives/content_aggregate.md (DOE directive)
- execution/content_aggregate.py (main orchestrator script)
- Both with DOE-VERSION: 2026.01.29
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@AGENTS.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create DOE Directive</name>
  <files>directives/content_aggregate.md</files>
  <action>
Create directives/content_aggregate.md:

```markdown
# Content Aggregation Workflow
<!-- DOE-VERSION: 2026.01.29 -->

## Goal
Pull trending content from e-commerce subreddits, calculate outlier scores, and store for newsletter curation.

## Trigger Phrases
- "aggregate content"
- "fetch reddit posts"
- "update content cache"
- "find viral reddit posts"

## Quick Start
```bash
# Fetch all subreddits, save to cache
python execution/content_aggregate.py

# Show only 3x+ outliers
python execution/content_aggregate.py --min-score 3.0

# Fetch from specific subreddits
python execution/content_aggregate.py --subreddits shopify,dropship

# Limit posts per subreddit
python execution/content_aggregate.py --limit 25
```

## What It Does
1. Connects to Reddit API via PRAW
2. Fetches top posts from r/shopify, r/dropship, r/ecommerce (past week)
3. Calculates subreddit average upvotes for baseline
4. Calculates outlier score: (upvotes / avg) * recency_boost * modifiers
5. Applies engagement modifiers:
   - +30% for money hooks ($, revenue, profit)
   - +20% for time hooks (minutes, fast, quick)
   - +20% for secrets hooks (secret, hidden)
   - +15% for controversy (unpopular opinion)
6. Saves all posts to `data/content_cache/reddit/` as JSON
7. Prints summary of high-scoring posts

## Output
- JSON file in `data/content_cache/reddit/reddit_YYYYMMDD_HHMMSS.json`
- Console output showing post count and top outliers
- Each post includes:
  - id, title, selftext, url
  - subreddit, upvotes, num_comments
  - outlier_score, subreddit_avg_upvotes
  - fetched_at timestamp

## Configuration
Required environment variables (in .env):
```
REDDIT_CLIENT_ID=your_client_id
REDDIT_CLIENT_SECRET=your_client_secret
REDDIT_USER_AGENT=dtc-newsletter-bot/1.0 by /u/yourusername
```

## Outlier Score Interpretation
| Score | Meaning |
|-------|---------|
| < 1.0 | Below average performance |
| 1.0-2.0 | Average performance |
| 2.0-3.0 | Good performance |
| 3.0-5.0 | Strong outlier (3-5x average) |
| 5.0+ | Viral content (5x+ average) |

## Dependencies
- praw>=7.7.0
- python-dotenv>=1.0.0

## Related Scripts
- `execution/reddit_fetcher.py` - Reddit API integration
- `execution/scoring.py` - Outlier score calculation
- `execution/storage.py` - JSON file storage
```

Ensure the directive follows the AGENTS.md structure (Goal, Trigger Phrases, Quick Start, What It Does, Output).
  </action>
  <verify>
```bash
ls -la directives/content_aggregate.md
grep "DOE-VERSION: 2026.01.29" directives/content_aggregate.md
```
  </verify>
  <done>Directive exists with correct DOE version and all required sections</done>
</task>

<task type="auto">
  <name>Task 2: Create Main Script</name>
  <files>execution/content_aggregate.py</files>
  <action>
Create execution/content_aggregate.py:

```python
#!/usr/bin/env python3
"""
Content Aggregation - Reddit Outlier Detection

Fetches trending posts from e-commerce subreddits, calculates outlier scores,
and stores for newsletter curation.

DOE-VERSION: 2026.01.29

Usage:
    python execution/content_aggregate.py [options]

Options:
    --min-score FLOAT    Minimum outlier score to display (default: 0)
    --limit INT          Posts per subreddit (default: 50)
    --subreddits LIST    Comma-separated subreddits (default: shopify,dropship,ecommerce)
    --no-save            Don't save to cache, just print
    --help               Show this help message
"""
import argparse
import sys
from typing import List, Optional

from execution.reddit_fetcher import fetch_all_subreddits, TARGET_SUBREDDITS
from execution.storage import save_reddit_posts, get_high_outlier_posts


def format_post_summary(post: dict) -> str:
    """Format a post for console output."""
    score = post.get("outlier_score", 0)
    title = post.get("title", "")[:60]
    if len(post.get("title", "")) > 60:
        title += "..."
    subreddit = post.get("subreddit", "unknown")
    upvotes = post.get("upvotes", 0)
    
    return f"  [{score:>5.1f}x] r/{subreddit:<12} | {upvotes:>5} upvotes | {title}"


def main(
    min_score: float = 0.0,
    limit: int = 50,
    subreddits: Optional[List[str]] = None,
    save: bool = True
) -> int:
    """
    Main aggregation workflow.
    
    Args:
        min_score: Minimum outlier score to display
        limit: Posts per subreddit
        subreddits: List of subreddits to fetch
        save: Whether to save to cache
    
    Returns:
        Exit code (0 for success)
    """
    if subreddits is None:
        subreddits = TARGET_SUBREDDITS
    
    print(f"\n{'='*60}")
    print("CONTENT AGGREGATION - Reddit Outlier Detection")
    print(f"{'='*60}")
    print(f"Subreddits: {', '.join(f'r/{s}' for s in subreddits)}")
    print(f"Limit: {limit} posts per subreddit")
    print(f"Min score filter: {min_score}x")
    print()
    
    try:
        # Fetch posts
        print("Fetching posts from Reddit...")
        posts = fetch_all_subreddits(
            subreddits=subreddits,
            limit_per_sub=limit,
            min_outlier_score=0  # Get all, filter later for display
        )
        print(f"Fetched {len(posts)} total posts\n")
        
        if not posts:
            print("No posts found. Check your Reddit credentials.")
            return 1
        
        # Save to cache
        if save:
            filepath = save_reddit_posts(posts)
            print(f"Saved to: {filepath}\n")
        
        # Filter for display
        display_posts = [p for p in posts if p.get("outlier_score", 0) >= min_score]
        
        # Show summary
        print(f"Posts with outlier score >= {min_score}x: {len(display_posts)}")
        print("-" * 60)
        
        if display_posts:
            for post in display_posts[:20]:  # Top 20
                print(format_post_summary(post))
            
            if len(display_posts) > 20:
                print(f"\n  ... and {len(display_posts) - 20} more")
        else:
            print("  No posts meet the minimum score threshold.")
        
        # Stats
        print("\n" + "-" * 60)
        print("STATISTICS:")
        high_outliers = [p for p in posts if p.get("outlier_score", 0) >= 3.0]
        viral = [p for p in posts if p.get("outlier_score", 0) >= 5.0]
        print(f"  3x+ outliers: {len(high_outliers)}")
        print(f"  5x+ viral:    {len(viral)}")
        
        if posts:
            avg_score = sum(p.get("outlier_score", 0) for p in posts) / len(posts)
            max_score = max(p.get("outlier_score", 0) for p in posts)
            print(f"  Avg score:    {avg_score:.2f}x")
            print(f"  Max score:    {max_score:.2f}x")
        
        print(f"\n{'='*60}\n")
        return 0
        
    except ValueError as e:
        print(f"Configuration error: {e}")
        return 1
    except Exception as e:
        print(f"Error: {e}")
        return 1


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Fetch Reddit posts and calculate outlier scores"
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=0.0,
        help="Minimum outlier score to display (default: 0)"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=50,
        help="Posts per subreddit (default: 50)"
    )
    parser.add_argument(
        "--subreddits",
        type=str,
        default=None,
        help="Comma-separated subreddit names (default: shopify,dropship,ecommerce)"
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="Don't save to cache, just print"
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    
    subreddits = None
    if args.subreddits:
        subreddits = [s.strip() for s in args.subreddits.split(",")]
    
    exit_code = main(
        min_score=args.min_score,
        limit=args.limit,
        subreddits=subreddits,
        save=not args.no_save
    )
    sys.exit(exit_code)
```

Ensure the script has the DOE-VERSION comment matching the directive.
  </action>
  <verify>
```bash
# Check DOE version matches
grep "DOE-VERSION: 2026.01.29" execution/content_aggregate.py
grep "DOE-VERSION: 2026.01.29" directives/content_aggregate.md

# Verify script runs (will fail without credentials, but should parse)
python execution/content_aggregate.py --help
```
  </verify>
  <done>
- Script has matching DOE-VERSION: 2026.01.29
- --help shows usage correctly
- Script imports from reddit_fetcher and storage modules
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify End-to-End Pipeline</name>
  <files>data/content_cache/reddit/</files>
  <action>
Run the complete pipeline and verify:

1. Execute content aggregation (requires .env with Reddit credentials from Plan 02)
2. Check JSON output file is created
3. Verify posts have all required fields
4. Test filtering to 3x+ outliers

```bash
# Run full aggregation
python execution/content_aggregate.py --limit 25

# Show only 3x+ outliers
python execution/content_aggregate.py --min-score 3.0 --no-save

# Verify JSON structure
python -c "
import json
from pathlib import Path

files = sorted(Path('data/content_cache/reddit').glob('*.json'), reverse=True)
if files:
    with open(files[0]) as f:
        data = json.load(f)
    print(f'Latest file: {files[0].name}')
    print(f'Post count: {data[\"post_count\"]}')
    if data['posts']:
        p = data['posts'][0]
        required = ['id', 'title', 'url', 'upvotes', 'outlier_score', 'subreddit']
        missing = [k for k in required if k not in p]
        if missing:
            print(f'Missing fields: {missing}')
        else:
            print('All required fields present')
        print(f'Top post: [{p[\"outlier_score\"]:.1f}x] {p[\"title\"][:50]}')
else:
    print('No cache files found')
"
```

If any issues, debug and fix.
  </action>
  <verify>
```bash
# Verify cache has content
ls -la data/content_cache/reddit/*.json 2>/dev/null | head -3

# Verify 3x filtering works
python execution/content_aggregate.py --min-score 3.0 --limit 10 --no-save
```
  </verify>
  <done>
- Full pipeline runs successfully
- JSON files created with all required fields
- 3x+ filtering shows only high outliers
- DOE directive and script are paired with matching versions
  </done>
</task>

</tasks>

<verification>
```bash
# DOE version match
grep -h "DOE-VERSION" directives/content_aggregate.md execution/content_aggregate.py

# Script runnable
python execution/content_aggregate.py --help

# Full pipeline (requires credentials)
python execution/content_aggregate.py --limit 10

# Cache populated
ls data/content_cache/reddit/
```
</verification>

<success_criteria>
1. directives/content_aggregate.md exists with DOE-VERSION: 2026.01.29
2. execution/content_aggregate.py exists with matching DOE-VERSION
3. python execution/content_aggregate.py runs full pipeline
4. --min-score 3.0 filters to 3x+ outliers only
5. JSON files in data/content_cache/reddit/ have all required fields
6. Phase 1 requirements AGGR-02, AGGR-04, AGGR-05, AGGR-06 are complete
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
