---
phase: 03-stretch-sources
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - .env.example
  - execution/apify_base.py
  - execution/twitter_aggregate.py
  - directives/twitter_aggregate.md
  - tests/test_twitter_aggregate.py
autonomous: true

must_haves:
  truths:
    - "Apify client connects and can invoke actors"
    - "Twitter fetcher returns tweets with engagement data or graceful failure"
    - "Outlier scoring works for Twitter posts"
    - "Graceful degradation logs warning but continues"
  artifacts:
    - path: "requirements.txt"
      provides: "Apify and retry dependencies"
      contains: "apify-client"
    - path: "execution/apify_base.py"
      provides: "Shared Apify utilities"
      exports: ["fetch_from_apify", "fetch_with_retry"]
    - path: "execution/twitter_aggregate.py"
      provides: "Twitter/X data fetcher"
      exports: ["fetch_dtc_tweets", "score_twitter_post"]
    - path: "directives/twitter_aggregate.md"
      provides: "DOE directive for Twitter"
      contains: "DOE-VERSION: 2026.01.31"
  key_links:
    - from: "execution/twitter_aggregate.py"
      to: "execution/apify_base.py"
      via: "import"
      pattern: "from execution.apify_base import"
    - from: "execution/twitter_aggregate.py"
      to: "execution/scoring.py"
      via: "scoring reuse"
      pattern: "from execution.scoring import"
---

<objective>
Set up Apify foundation and implement Twitter/X aggregator as first stretch source.

Purpose: Twitter provides unique signals (founder takes, controversy, product announcements) not available from Reddit. This plan establishes the Apify infrastructure all stretch sources will use.

Output:
- Shared Apify utilities (connection, retry logic, caching)
- Working Twitter aggregator with composite scoring
- DOE directive matching script version
</objective>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-stretch-sources/03-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Apify Foundation Setup</name>
  <files>requirements.txt, .env.example, execution/apify_base.py</files>
  <action>
Update requirements.txt to add:
```
# Apify (stretch sources)
apify-client>=1.6.0
tenacity>=8.2.0
cachetools>=5.3.0
```

Update .env.example to add:
```
# Apify API (https://console.apify.com/account/integrations)
APIFY_TOKEN=your_apify_token
```

Create execution/apify_base.py:
```python
"""
Shared Apify utilities for stretch source aggregators.
DOE-VERSION: 2026.01.31

Provides:
- Base actor invocation with error handling
- Retry logic with exponential backoff
- TTL caching for results (24 hours)
"""

import os
import logging
from typing import Any, Callable
from datetime import datetime, timezone

from apify_client import ApifyClient
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from cachetools import TTLCache

logger = logging.getLogger(__name__)

# 24-hour cache for stretch source results
_cache = TTLCache(maxsize=100, ttl=24 * 60 * 60)


def get_apify_client() -> ApifyClient:
    """
    Get configured Apify client.
    
    Raises:
        ValueError: If APIFY_TOKEN not configured
    """
    token = os.getenv("APIFY_TOKEN")
    if not token:
        raise ValueError(
            "APIFY_TOKEN not configured. "
            "Get your token at https://console.apify.com/account/integrations"
        )
    return ApifyClient(token)


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type(Exception),
    reraise=True,
)
def fetch_from_apify(actor_id: str, run_input: dict) -> list[dict]:
    """
    Invoke an Apify actor and return results.
    
    Args:
        actor_id: Actor identifier (e.g., "apidojo/tweet-scraper")
        run_input: Input parameters for the actor
    
    Returns:
        List of result items from the actor run
    
    Raises:
        Exception: On actor failure after retries
    """
    client = get_apify_client()
    
    logger.info(f"Starting Apify actor: {actor_id}")
    run = client.actor(actor_id).call(run_input=run_input)
    
    items = list(client.dataset(run["defaultDatasetId"]).iterate_items())
    logger.info(f"Actor {actor_id} returned {len(items)} items")
    
    return items


def fetch_with_retry(
    source_name: str,
    fetch_fn: Callable[[], list[dict]],
    cache_key: str | None = None,
) -> dict:
    """
    Execute a fetch function with retry and graceful degradation.
    
    Args:
        source_name: Name for logging (e.g., "twitter")
        fetch_fn: Function that returns list of items
        cache_key: Optional key for caching results
    
    Returns:
        Dict with keys: success, items, error, cached, timestamp
    """
    # Check cache first
    if cache_key and cache_key in _cache:
        logger.info(f"{source_name}: returning cached results")
        return {
            "success": True,
            "items": _cache[cache_key],
            "error": None,
            "cached": True,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
    
    try:
        items = fetch_fn()
        
        # Cache results
        if cache_key:
            _cache[cache_key] = items
        
        return {
            "success": True,
            "items": items,
            "error": None,
            "cached": False,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }
        
    except Exception as e:
        logger.warning(f"{source_name} failed after retries: {e}")
        return {
            "success": False,
            "items": [],
            "error": str(e),
            "cached": False,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }


def clear_cache() -> None:
    """Clear the TTL cache (useful for testing)."""
    _cache.clear()
```
  </action>
  <verify>
```bash
pip install apify-client tenacity cachetools
python -c "from execution.apify_base import fetch_from_apify, fetch_with_retry; print('Apify base OK')"
```
  </verify>
  <done>
- requirements.txt includes apify-client, tenacity, cachetools
- .env.example documents APIFY_TOKEN
- apify_base.py exports fetch_from_apify, fetch_with_retry
- Module imports without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Twitter/X Aggregator</name>
  <files>execution/twitter_aggregate.py, directives/twitter_aggregate.md, tests/test_twitter_aggregate.py</files>
  <action>
Create execution/twitter_aggregate.py:
```python
#!/usr/bin/env python3
"""
Twitter/X content aggregator for DTC Newsletter.
DOE-VERSION: 2026.01.31

Fetches viral tweets from DTC founders and brands using Apify.
Surfaces founder takes, product announcements, and controversy signals.
"""

import argparse
import os
import sys
import logging
from datetime import datetime, timezone

# Add parent directory to path for direct script execution
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

from execution.apify_base import fetch_from_apify, fetch_with_retry

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Curated DTC accounts and search terms
DTC_SEARCH_TERMS = [
    "shopify founder",
    "dtc brand launch", 
    "ecommerce revenue",
    "dropshipping 2026",
    "#dtcbrand viral",
]

DTC_ACCOUNTS = [
    "toikishan",       # Shopify founder
    "levelsio",        # Indie maker
    "taborpitcher",    # DTC operator
    "RyanReynolds",    # Celebrity DTC
]

TWITTER_ACTOR = "apidojo/tweet-scraper"


def score_twitter_post(tweet: dict, account_avg_engagement: float = 1000.0) -> dict:
    """
    Calculate outlier score for a tweet.
    
    Composite score combining:
    - Engagement ratio (likes + retweets + quotes vs account average)
    - Quote tweet ratio boost (high quotes = controversial/viral)
    
    Args:
        tweet: Raw tweet data from Apify
        account_avg_engagement: Historical average for comparison
    
    Returns:
        Tweet dict with outlier_score and engagement breakdown added
    """
    likes = tweet.get("likeCount", 0) or 0
    retweets = tweet.get("retweetCount", 0) or 0
    quotes = tweet.get("quoteCount", 0) or 0
    replies = tweet.get("replyCount", 0) or 0
    
    # Total engagement
    engagement = likes + retweets + quotes + replies
    
    # Base ratio vs account average
    base_ratio = engagement / max(account_avg_engagement, 1)
    
    # Quote boost: high quote ratio indicates controversy/discussion
    # Quotes > 30% of retweets = 1.3x boost, else 1.0x
    quote_boost = 1.3 if retweets > 0 and quotes > retweets * 0.3 else 1.0
    
    # Final score
    outlier_score = base_ratio * quote_boost
    
    return {
        **tweet,
        "source": "twitter",
        "outlier_score": round(outlier_score, 2),
        "engagement": {
            "likes": likes,
            "retweets": retweets,
            "quotes": quotes,
            "replies": replies,
            "total": engagement,
        },
        "url": f"https://twitter.com/{tweet.get('author', {}).get('userName', '')}/status/{tweet.get('id', '')}",
    }


def fetch_dtc_tweets(
    search_terms: list[str] | None = None,
    max_per_term: int = 50,
) -> list[dict]:
    """
    Fetch tweets matching DTC search terms via Apify.
    
    Args:
        search_terms: List of search queries (defaults to DTC_SEARCH_TERMS)
        max_per_term: Max tweets per search term
    
    Returns:
        List of scored tweet dicts
    """
    terms = search_terms or DTC_SEARCH_TERMS
    
    all_tweets = []
    seen_ids = set()
    
    for term in terms:
        logger.info(f"Searching Twitter for: {term}")
        
        run_input = {
            "searchTerms": [term],
            "sort": "Latest",
            "maxItems": max_per_term,
        }
        
        try:
            items = fetch_from_apify(TWITTER_ACTOR, run_input)
            
            for tweet in items:
                tweet_id = tweet.get("id")
                if tweet_id and tweet_id not in seen_ids:
                    seen_ids.add(tweet_id)
                    scored = score_twitter_post(tweet)
                    all_tweets.append(scored)
                    
        except Exception as e:
            logger.warning(f"Failed to fetch tweets for '{term}': {e}")
            continue
    
    # Sort by outlier score descending
    all_tweets.sort(key=lambda x: x.get("outlier_score", 0), reverse=True)
    
    return all_tweets


def run_twitter_aggregation(
    min_score: float = 2.0,
    max_per_term: int = 50,
) -> dict:
    """
    Run full Twitter aggregation with graceful degradation.
    
    Args:
        min_score: Minimum outlier score to include in results
        max_per_term: Max tweets per search term
    
    Returns:
        Dict with success status, tweets, and metadata
    """
    start_time = datetime.now(timezone.utc)
    
    print("\n=== Twitter/X Aggregation ===")
    print(f"Started: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")
    print(f"Search terms: {len(DTC_SEARCH_TERMS)}")
    print(f"Minimum outlier score: {min_score}x")
    print("-" * 40)
    
    result = fetch_with_retry(
        source_name="twitter",
        fetch_fn=lambda: fetch_dtc_tweets(max_per_term=max_per_term),
        cache_key="twitter_dtc_tweets",
    )
    
    if not result["success"]:
        print(f"\nTwitter fetch failed: {result['error']}")
        print("Pipeline will continue without Twitter data.")
        return result
    
    tweets = result["items"]
    
    # Filter by minimum score
    high_score_tweets = [t for t in tweets if t.get("outlier_score", 0) >= min_score]
    
    print(f"\nFetched: {len(tweets)} tweets")
    print(f"Above {min_score}x threshold: {len(high_score_tweets)}")
    
    if high_score_tweets:
        print(f"\nTop 5 tweets:")
        for i, tweet in enumerate(high_score_tweets[:5], 1):
            score = tweet.get("outlier_score", 0)
            text = tweet.get("text", "")[:80] + "..." if len(tweet.get("text", "")) > 80 else tweet.get("text", "")
            author = tweet.get("author", {}).get("userName", "unknown")
            print(f"  {i}. [{score:.1f}x] @{author}: {text}")
    
    end_time = datetime.now(timezone.utc)
    duration = (end_time - start_time).total_seconds()
    
    print(f"\nCompleted in {duration:.1f}s")
    print("-" * 40)
    
    return {
        **result,
        "items": high_score_tweets,
        "total_fetched": len(tweets),
        "duration_seconds": duration,
    }


def parse_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Aggregate trending tweets from DTC accounts and topics"
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=2.0,
        help="Minimum outlier score (default: 2.0)",
    )
    parser.add_argument(
        "--max-per-term",
        type=int,
        default=50,
        help="Max tweets per search term (default: 50)",
    )
    return parser.parse_args()


def main() -> int:
    """Main entry point."""
    args = parse_args()
    result = run_twitter_aggregation(
        min_score=args.min_score,
        max_per_term=args.max_per_term,
    )
    return 0 if result.get("success") else 1


if __name__ == "__main__":
    sys.exit(main())
```

Create directives/twitter_aggregate.md:
```markdown
# Twitter/X Content Aggregation
<!-- DOE-VERSION: 2026.01.31 -->

## Goal
Fetch viral tweets from DTC founders and brands to surface unique signals not found on Reddit.

## Trigger Phrases
- "get twitter trends"
- "fetch dtc tweets"
- "twitter aggregation"
- "run twitter aggregate"

## Quick Start
```bash
python execution/twitter_aggregate.py
python execution/twitter_aggregate.py --min-score 3.0
```

## What It Does
1. Searches Twitter for DTC-relevant terms via Apify
2. Calculates outlier scores (engagement vs account average)
3. Applies quote boost for controversial/viral content
4. Returns scored tweets sorted by outlier score

## CLI Options
| Flag | Default | Description |
|------|---------|-------------|
| --min-score | 2.0 | Minimum outlier score threshold |
| --max-per-term | 50 | Max tweets per search term |

## Output
- Console display of top tweets with scores
- Returns dict with: success, items, total_fetched, duration_seconds
- Graceful degradation on failure (logs warning, returns empty)

## Unique Value
Twitter surfaces:
- Founder hot takes and opinions
- Product launch announcements
- Controversy and drama (high quote ratio)
- Viral threads with tactical advice
```

Create tests/test_twitter_aggregate.py with scoring tests.
  </action>
  <verify>
```bash
python -c "from execution.twitter_aggregate import score_twitter_post, fetch_dtc_tweets; print('Twitter module OK')"
python -m pytest tests/test_twitter_aggregate.py -v 2>/dev/null || echo "Tests created, run with APIFY_TOKEN for integration"
```
  </verify>
  <done>
- twitter_aggregate.py imports and exports correctly
- score_twitter_post calculates composite scores
- DOE directive matches script version (2026.01.31)
- Graceful degradation on API failure
  </done>
</task>

</tasks>

<verification>
```bash
# Dependencies installed
pip list | grep -E "apify-client|tenacity|cachetools"

# Modules import correctly
python -c "from execution.apify_base import fetch_from_apify; print('OK')"
python -c "from execution.twitter_aggregate import run_twitter_aggregation; print('OK')"

# DOE versions match
grep "DOE-VERSION: 2026.01.31" execution/twitter_aggregate.py
grep "DOE-VERSION: 2026.01.31" directives/twitter_aggregate.md
```
</verification>

<success_criteria>
1. requirements.txt includes apify-client>=1.6.0, tenacity>=8.2.0, cachetools>=5.3.0
2. .env.example documents APIFY_TOKEN
3. execution/apify_base.py exports fetch_from_apify, fetch_with_retry
4. execution/twitter_aggregate.py runs and returns results (or graceful failure)
5. directives/twitter_aggregate.md has matching DOE-VERSION: 2026.01.31
6. Scoring produces meaningful outlier scores with quote boost
</success_criteria>

<output>
After completion, create `.planning/phases/03-stretch-sources/03-01-SUMMARY.md`
</output>
