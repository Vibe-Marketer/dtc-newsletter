---
phase: 02-core-sources
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - execution/perplexity_client.py
  - execution/deduplication.py
  - tests/test_perplexity_client.py
  - tests/test_deduplication.py
  - data/content_cache/perplexity/.gitkeep
autonomous: true

user_setup:
  - service: perplexity
    why: "Web-grounded research API for trend summaries"
    env_vars:
      - name: PERPLEXITY_API_KEY
        source: "https://www.perplexity.ai/settings/api â†’ API Keys"

must_haves:
  truths:
    - "Perplexity client returns web-grounded trend research with citations"
    - "Deduplication prevents repeating content from last 4 weeks"
    - "Both modules follow existing codebase patterns (scoring.py, storage.py)"
  artifacts:
    - path: "execution/perplexity_client.py"
      provides: "Perplexity API integration"
      exports: ["search_trends", "deep_dive_topic"]
      min_lines: 80
    - path: "execution/deduplication.py"
      provides: "Hash-based content deduplication"
      exports: ["generate_content_hash", "is_duplicate", "load_seen_hashes"]
      min_lines: 50
    - path: "tests/test_perplexity_client.py"
      provides: "Perplexity client tests"
      min_lines: 40
    - path: "tests/test_deduplication.py"
      provides: "Deduplication tests"
      min_lines: 40
  key_links:
    - from: "execution/perplexity_client.py"
      to: "PERPLEXITY_API_KEY"
      via: "os.getenv"
      pattern: 'os\\.getenv.*PERPLEXITY'
    - from: "execution/deduplication.py"
      to: "data/content_cache/"
      via: "hash file loading"
      pattern: "content_cache"
---

<objective>
Create Perplexity API client for web-grounded research and hash-based deduplication module to prevent repeating content from last 4 weeks.

Purpose: Perplexity provides real-time web research with citations for trend summaries. Deduplication ensures newsletter doesn't repeat stories. Both modules are independent of TubeLab decision and can be built in parallel with Plan 01.

Output: Two tested modules ready for integration in Plan 04.
</objective>

<execution_context>
@~/.config/Claude/get-shit-done/workflows/execute-plan.md
@~/.config/Claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/02-core-sources/02-CONTEXT.md
@.planning/phases/02-core-sources/02-RESEARCH.md
@execution/storage.py
@execution/scoring.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Perplexity client module</name>
  <files>execution/perplexity_client.py, tests/test_perplexity_client.py, data/content_cache/perplexity/.gitkeep</files>
  <action>
Create `execution/perplexity_client.py` following the pattern from 02-RESEARCH.md:

```python
"""
Perplexity API client for web-grounded research.
DOE-VERSION: 2026.01.31

Two-stage research pattern:
1. search_trends() - Get trending e-commerce topics with citations
2. deep_dive_topic() - Detailed analysis of specific topic
"""

import os
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

# Install: pip install openai (Perplexity uses OpenAI-compatible API)
from openai import OpenAI

PERPLEXITY_BASE_URL = "https://api.perplexity.ai"
DEFAULT_MODEL = "sonar-pro"  # Best quality per user decision

def get_client() -> OpenAI:
    """Initialize Perplexity client using OpenAI-compatible API."""
    api_key = os.getenv("PERPLEXITY_API_KEY")
    if not api_key:
        raise ValueError("PERPLEXITY_API_KEY environment variable required")
    return OpenAI(api_key=api_key, base_url=PERPLEXITY_BASE_URL)

def search_trends(topic: str = "e-commerce DTC") -> dict:
    """
    Stage 1: Get trending topics with citations.
    
    Returns dict with:
    - content: Summary text
    - citations: List of source URLs
    - model: Model used
    - fetched_at: ISO timestamp
    """
    # Implementation: call client.chat.completions.create with sonar-pro
    # Return structured result
    pass

def deep_dive_topic(topic: str) -> dict:
    """
    Stage 2: Deep dive on specific topic.
    
    Returns same structure as search_trends but with detailed analysis.
    """
    pass

def save_research(research: dict, topic_slug: str) -> Path:
    """Save research to cache directory."""
    cache_dir = Path("data/content_cache/perplexity")
    cache_dir.mkdir(parents=True, exist_ok=True)
    # Save with date prefix
    pass
```

Implementation requirements:
- Use OpenAI Python SDK (Perplexity is OpenAI-compatible)
- Model: "sonar-pro" for best quality
- Include recency filter: last week for search_trends
- Return structured dict with content, citations, model, timestamp
- Handle API errors gracefully with clear messages
- Create data/content_cache/perplexity/.gitkeep for directory structure

Create `tests/test_perplexity_client.py`:
- Test get_client raises ValueError when no API key
- Test search_trends with mocked API response
- Test deep_dive_topic with mocked API response
- Test save_research creates file correctly
  </action>
  <verify>
```bash
# Check module exists and has required functions
python -c "from execution.perplexity_client import search_trends, deep_dive_topic, save_research; print('Imports OK')"

# Run tests
python -m pytest tests/test_perplexity_client.py -v
```
  </verify>
  <done>
- perplexity_client.py exists with search_trends, deep_dive_topic, save_research
- Tests pass with mocked API responses
- data/content_cache/perplexity/ directory exists
  </done>
</task>

<task type="auto">
  <name>Task 2: Create deduplication module</name>
  <files>execution/deduplication.py, tests/test_deduplication.py</files>
  <action>
Create `execution/deduplication.py` following pattern from 02-RESEARCH.md:

```python
"""
Content deduplication for DTC Newsletter.
DOE-VERSION: 2026.01.31

Hash-based deduplication prevents repeating content from last 4 weeks.
Works across all content sources (Reddit, YouTube, Perplexity).
"""

import hashlib
import json
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional

def generate_content_hash(content: dict) -> str:
    """
    Generate hash from content identifiers.
    
    Uses source + id for uniqueness:
    - Reddit: "reddit:{post_id}"
    - YouTube: "youtube:{video_id}"
    - Perplexity: "perplexity:{topic_slug}:{date}"
    
    Returns MD5 hex digest.
    """
    source = content.get("source", "unknown")
    content_id = content.get("id", content.get("video_id", content.get("post_id", "")))
    key = f"{source}:{content_id}"
    return hashlib.md5(key.encode()).hexdigest()

def load_seen_hashes(
    cache_dirs: list[Path] | None = None,
    weeks_back: int = 4
) -> set[str]:
    """
    Load content hashes from last N weeks across all cache directories.
    
    Scans all JSON files in cache directories, extracts IDs, generates hashes.
    
    Args:
        cache_dirs: List of cache directories to scan (default: all sources)
        weeks_back: Number of weeks to look back (default: 4)
    
    Returns:
        Set of content hashes seen in the time window
    """
    if cache_dirs is None:
        cache_dirs = [
            Path("data/content_cache/reddit"),
            Path("data/content_cache/youtube"),
            Path("data/content_cache/perplexity"),
        ]
    
    cutoff = datetime.now(timezone.utc) - timedelta(weeks=weeks_back)
    seen = set()
    
    # Implementation: scan JSON files, parse dates, extract content, hash
    pass

def is_duplicate(content: dict, seen_hashes: set[str]) -> bool:
    """Check if content was seen before."""
    return generate_content_hash(content) in seen_hashes

def filter_duplicates(
    contents: list[dict],
    weeks_back: int = 4
) -> tuple[list[dict], int]:
    """
    Filter out duplicate content.
    
    Returns:
        Tuple of (filtered_list, duplicate_count)
    """
    seen = load_seen_hashes(weeks_back=weeks_back)
    filtered = []
    duplicates = 0
    
    for content in contents:
        if is_duplicate(content, seen):
            duplicates += 1
        else:
            filtered.append(content)
    
    return filtered, duplicates
```

Implementation requirements:
- MD5 hash for speed (not cryptographic, just dedup)
- Scan all content sources (reddit, youtube, perplexity)
- Filter by file modification date for weeks_back
- Return set of hashes for efficiency

Create `tests/test_deduplication.py`:
- Test generate_content_hash produces consistent hashes
- Test different sources produce different hashes for same ID
- Test load_seen_hashes with mock cache files
- Test is_duplicate correctly identifies duplicates
- Test filter_duplicates removes known content
  </action>
  <verify>
```bash
# Check module exists and has required functions
python -c "from execution.deduplication import generate_content_hash, is_duplicate, load_seen_hashes, filter_duplicates; print('Imports OK')"

# Run tests
python -m pytest tests/test_deduplication.py -v
```
  </verify>
  <done>
- deduplication.py exists with generate_content_hash, is_duplicate, load_seen_hashes, filter_duplicates
- Tests pass
- Hash function is consistent and source-aware
  </done>
</task>

</tasks>

<verification>
1. `python -c "from execution.perplexity_client import search_trends, deep_dive_topic"` succeeds
2. `python -c "from execution.deduplication import filter_duplicates, is_duplicate"` succeeds
3. `python -m pytest tests/test_perplexity_client.py tests/test_deduplication.py -v` all pass
4. data/content_cache/perplexity/ directory exists
</verification>

<success_criteria>
- Perplexity client can make API calls with proper error handling
- Deduplication correctly hashes content across sources
- Both modules have passing tests
- Modules ready for integration in Plan 04
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-sources/02-02-SUMMARY.md`
</output>
