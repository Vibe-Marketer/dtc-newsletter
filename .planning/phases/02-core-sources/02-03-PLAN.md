---
phase: 02-core-sources
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - execution/youtube_fetcher.py
  - execution/transcript_fetcher.py
  - tests/test_youtube_fetcher.py
  - tests/test_transcript_fetcher.py
  - data/content_cache/youtube/.gitkeep
  - data/content_cache/transcripts/.gitkeep
autonomous: true

must_haves:
  truths:
    - "YouTube fetcher returns videos with outlier scores > 5x channel average"
    - "Transcripts fetched for top 10 high-scoring videos"
    - "Fetcher works with either TubeLab API or YouTube Data API (based on Plan 01 decision)"
  artifacts:
    - path: "execution/youtube_fetcher.py"
      provides: "YouTube video fetching with outlier detection"
      exports: ["fetch_channel_videos", "calculate_channel_average", "YouTubeFetcher"]
      min_lines: 120
    - path: "execution/transcript_fetcher.py"
      provides: "YouTube transcript fetching"
      exports: ["fetch_transcript", "fetch_transcripts_batch"]
      min_lines: 60
    - path: "tests/test_youtube_fetcher.py"
      provides: "YouTube fetcher tests"
      min_lines: 60
    - path: "tests/test_transcript_fetcher.py"
      provides: "Transcript fetcher tests"
      min_lines: 40
  key_links:
    - from: "execution/youtube_fetcher.py"
      to: "execution/scoring.py"
      via: "outlier calculation"
      pattern: "calculate_recency_boost|calculate_engagement_modifiers"
    - from: "execution/transcript_fetcher.py"
      to: "youtube-transcript-api"
      via: "library import"
      pattern: "YouTubeTranscriptApi"
---

<objective>
Create YouTube video fetcher with outlier detection and transcript fetcher for top videos. Implementation adapts based on TubeLab decision from Plan 01.

Purpose: YouTube is the highest-value content source for DTC newsletter. Outlier detection finds actually viral content (not just recent). Transcripts enable AI to understand video content for newsletter generation.

Output: Two tested modules - youtube_fetcher.py (with outlier scoring) and transcript_fetcher.py (top 10 videos per run).
</objective>

<execution_context>
@~/.config/Claude/get-shit-done/workflows/execute-plan.md
@~/.config/Claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/02-core-sources/02-CONTEXT.md
@.planning/phases/02-core-sources/02-RESEARCH.md
@.planning/phases/02-core-sources/02-TUBELAB-DECISION.md
@execution/scoring.py
@execution/storage.py
@execution/reddit_fetcher.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create YouTube fetcher module</name>
  <files>execution/youtube_fetcher.py, tests/test_youtube_fetcher.py, data/content_cache/youtube/.gitkeep</files>
  <action>
Read 02-TUBELAB-DECISION.md to determine implementation approach.

Create `execution/youtube_fetcher.py`:

```python
"""
YouTube video fetcher with outlier detection.
DOE-VERSION: 2026.01.31

Fetches videos from DTC/e-commerce channels and calculates outlier scores.
Uses TubeLab API if configured, otherwise YouTube Data API with manual scoring.
"""

import os
import json
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional

# Conditional imports based on available API
try:
    from googleapiclient.discovery import build
    YOUTUBE_DATA_API_AVAILABLE = True
except ImportError:
    YOUTUBE_DATA_API_AVAILABLE = False

from execution.scoring import calculate_recency_boost, calculate_engagement_modifiers

# Default DTC/e-commerce channels (from 02-RESEARCH.md recommendations)
DEFAULT_CHANNELS = [
    # Channel IDs - to be filled with actual IDs during implementation
    # Format: {"name": "Channel Name", "id": "UCxxxxx"}
]

# Outlier threshold per CONTEXT.md: 5x channel average
MIN_OUTLIER_SCORE = 5.0

# Time window per CONTEXT.md: last 14 days
DAYS_LOOKBACK = 14


class YouTubeFetcher:
    """YouTube video fetcher with outlier detection."""
    
    def __init__(self, use_tubelab: bool = False):
        """
        Initialize fetcher.
        
        Args:
            use_tubelab: If True, use TubeLab API; otherwise YouTube Data API
        """
        self.use_tubelab = use_tubelab
        self._client = None
        
    def _get_tubelab_client(self):
        """Initialize TubeLab API client."""
        api_key = os.getenv("TUBELAB_API_KEY")
        if not api_key:
            raise ValueError("TUBELAB_API_KEY environment variable required")
        # TubeLab client implementation based on their API
        pass
        
    def _get_youtube_client(self):
        """Initialize YouTube Data API client."""
        api_key = os.getenv("YOUTUBE_API_KEY")
        if not api_key:
            raise ValueError("YOUTUBE_API_KEY environment variable required")
        if not YOUTUBE_DATA_API_AVAILABLE:
            raise ImportError("google-api-python-client required. Run: pip install google-api-python-client")
        return build("youtube", "v3", developerKey=api_key)
    
    def fetch_channel_videos(
        self,
        channel_id: str,
        days_back: int = DAYS_LOOKBACK,
        limit: int = 50
    ) -> list[dict]:
        """Fetch recent videos from a channel."""
        pass
    
    def calculate_channel_average(self, channel_id: str, sample_size: int = 100) -> float:
        """Calculate average views for channel's last N videos."""
        pass
    
    def calculate_outlier_score(
        self,
        views: int,
        channel_avg: float,
        published_at: str,
        title: str,
        description: str = ""
    ) -> float:
        """
        Calculate outlier score for a video.
        
        Uses same formula as Reddit scoring:
        score = (views / channel_avg) * recency_boost * engagement_modifiers
        """
        pass
    
    def fetch_all_channels(
        self,
        channels: list[dict] | None = None,
        min_outlier_score: float = MIN_OUTLIER_SCORE
    ) -> list[dict]:
        """
        Fetch videos from all channels, filter by outlier score.
        
        Returns standardized video dicts with:
        - id, title, url, thumbnail_url
        - views, channel_name, channel_avg_views
        - outlier_score, engagement_modifiers
        - published_at, fetched_at, source: "youtube"
        """
        pass


def fetch_channel_videos(channel_id: str, **kwargs) -> list[dict]:
    """Convenience function using default fetcher."""
    fetcher = YouTubeFetcher()
    return fetcher.fetch_channel_videos(channel_id, **kwargs)


def save_youtube_videos(videos: list[dict], cache_dir: Path | None = None) -> Path:
    """Save videos to cache (same pattern as Reddit storage)."""
    cache_dir = cache_dir or Path("data/content_cache/youtube")
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"youtube_{datetime.now(timezone.utc).strftime('%Y-%m-%d')}.json"
    filepath = cache_dir / filename
    
    cache_data = {
        "metadata": {
            "source": "youtube",
            "fetched_at": datetime.now(timezone.utc).isoformat(),
            "video_count": len(videos),
        },
        "videos": videos,
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(cache_data, f, indent=2, ensure_ascii=False)
    
    return filepath
```

Implementation notes:
- If TubeLab decision was "youtube" (fallback): Use YouTube Data API with manual outlier scoring
- If TubeLab decision was "tubelab": Implement TubeLab API client (adapt based on 02-TUBELAB-DECISION.md endpoints)
- Reuse scoring.py functions for recency boost and engagement modifiers
- Handle API quota limits for YouTube Data API (10,000 units/day)
- Create data/content_cache/youtube/.gitkeep

Create `tests/test_youtube_fetcher.py`:
- Test YouTubeFetcher initialization (both modes)
- Test calculate_outlier_score with known values
- Test fetch_channel_videos with mocked API
- Test save_youtube_videos creates correct file structure
  </action>
  <verify>
```bash
# Check module exists
python -c "from execution.youtube_fetcher import YouTubeFetcher, fetch_channel_videos, save_youtube_videos; print('Imports OK')"

# Run tests
python -m pytest tests/test_youtube_fetcher.py -v
```
  </verify>
  <done>
- youtube_fetcher.py exists with YouTubeFetcher class and helper functions
- Outlier scoring uses existing scoring.py functions
- Tests pass with mocked API responses
- data/content_cache/youtube/ directory exists
  </done>
</task>

<task type="auto">
  <name>Task 2: Create transcript fetcher module</name>
  <files>execution/transcript_fetcher.py, tests/test_transcript_fetcher.py, data/content_cache/transcripts/.gitkeep</files>
  <action>
Create `execution/transcript_fetcher.py` using youtube-transcript-api:

```python
"""
YouTube transcript fetcher.
DOE-VERSION: 2026.01.31

Fetches transcripts for high-scoring YouTube videos.
Per CONTEXT.md: Fetch for top 10 high-scoring videos per run.
"""

import json
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound

# Per RESEARCH.md pitfall #1: delay to avoid IP bans
REQUEST_DELAY_SECONDS = 1.5

# Per CONTEXT.md: fetch for top 10 videos
DEFAULT_BATCH_SIZE = 10


def fetch_transcript(
    video_id: str,
    languages: list[str] | None = None
) -> dict:
    """
    Fetch transcript for a single video.
    
    Args:
        video_id: YouTube video ID (not full URL)
        languages: Preferred languages (default: ['en'])
    
    Returns:
        Dict with:
        - video_id: str
        - transcript: list[dict] with {text, start, duration}
        - language: str
        - is_generated: bool
        - fetched_at: ISO timestamp
        - error: str | None
    """
    if languages is None:
        languages = ['en']
    
    try:
        ytt_api = YouTubeTranscriptApi()
        transcript = ytt_api.fetch(video_id, languages=languages)
        
        return {
            "video_id": video_id,
            "transcript": transcript.to_raw_data(),
            "language": "en",  # or extract from result
            "is_generated": False,  # or extract from result
            "fetched_at": datetime.now(timezone.utc).isoformat(),
            "error": None,
        }
    except (TranscriptsDisabled, NoTranscriptFound) as e:
        return {
            "video_id": video_id,
            "transcript": [],
            "language": None,
            "is_generated": None,
            "fetched_at": datetime.now(timezone.utc).isoformat(),
            "error": str(e),
        }
    except Exception as e:
        return {
            "video_id": video_id,
            "transcript": [],
            "language": None,
            "is_generated": None,
            "fetched_at": datetime.now(timezone.utc).isoformat(),
            "error": f"Unexpected error: {e}",
        }


def fetch_transcripts_batch(
    video_ids: list[str],
    limit: int = DEFAULT_BATCH_SIZE,
    delay: float = REQUEST_DELAY_SECONDS
) -> list[dict]:
    """
    Fetch transcripts for multiple videos with rate limiting.
    
    Args:
        video_ids: List of video IDs
        limit: Max videos to fetch (default: 10)
        delay: Seconds between requests (default: 1.5)
    
    Returns:
        List of transcript results (success and failures)
    """
    results = []
    
    for i, video_id in enumerate(video_ids[:limit]):
        if i > 0:
            time.sleep(delay)
        
        result = fetch_transcript(video_id)
        results.append(result)
        
        # Log progress
        status = "OK" if result["error"] is None else f"SKIP: {result['error']}"
        print(f"  [{i+1}/{min(len(video_ids), limit)}] {video_id}: {status}")
    
    return results


def save_transcripts(
    transcripts: list[dict],
    cache_dir: Path | None = None
) -> Path:
    """Save transcripts to cache directory."""
    cache_dir = cache_dir or Path("data/content_cache/transcripts")
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"transcripts_{datetime.now(timezone.utc).strftime('%Y-%m-%d')}.json"
    filepath = cache_dir / filename
    
    cache_data = {
        "metadata": {
            "source": "youtube_transcripts",
            "fetched_at": datetime.now(timezone.utc).isoformat(),
            "total_videos": len(transcripts),
            "successful": sum(1 for t in transcripts if t["error"] is None),
            "failed": sum(1 for t in transcripts if t["error"] is not None),
        },
        "transcripts": transcripts,
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(cache_data, f, indent=2, ensure_ascii=False)
    
    return filepath


def get_transcript_text(transcript_data: dict) -> str:
    """Extract plain text from transcript data."""
    if not transcript_data.get("transcript"):
        return ""
    
    return " ".join(
        segment.get("text", "")
        for segment in transcript_data["transcript"]
    )
```

Implementation notes:
- Install: pip install youtube-transcript-api
- Handle TranscriptsDisabled, NoTranscriptFound gracefully (per RESEARCH.md pitfall #3)
- Rate limit requests to avoid IP bans (per RESEARCH.md pitfall #1)
- Create data/content_cache/transcripts/.gitkeep

Create `tests/test_transcript_fetcher.py`:
- Test fetch_transcript with mocked YouTubeTranscriptApi
- Test error handling for TranscriptsDisabled, NoTranscriptFound
- Test fetch_transcripts_batch respects limit
- Test save_transcripts creates correct file structure
- Test get_transcript_text extracts text correctly
  </action>
  <verify>
```bash
# Check module exists
python -c "from execution.transcript_fetcher import fetch_transcript, fetch_transcripts_batch, save_transcripts, get_transcript_text; print('Imports OK')"

# Run tests
python -m pytest tests/test_transcript_fetcher.py -v
```
  </verify>
  <done>
- transcript_fetcher.py exists with all functions
- Error handling for missing/disabled transcripts
- Rate limiting implemented
- Tests pass
- data/content_cache/transcripts/ directory exists
  </done>
</task>

</tasks>

<verification>
1. `python -c "from execution.youtube_fetcher import YouTubeFetcher"` succeeds
2. `python -c "from execution.transcript_fetcher import fetch_transcripts_batch"` succeeds
3. `python -m pytest tests/test_youtube_fetcher.py tests/test_transcript_fetcher.py -v` all pass
4. Both modules use existing scoring.py patterns
5. Cache directories exist with .gitkeep files
</verification>

<success_criteria>
- YouTube fetcher works with configured API (TubeLab or YouTube Data API)
- Outlier scores calculated using same formula as Reddit (views/avg * recency * modifiers)
- Transcripts fetched with proper rate limiting and error handling
- Both modules have passing tests
- Modules ready for integration in Plan 04
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-sources/02-03-SUMMARY.md`
</output>
