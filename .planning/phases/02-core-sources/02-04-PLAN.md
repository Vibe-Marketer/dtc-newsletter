---
phase: 02-core-sources
plan: 04
type: execute
wave: 3
depends_on: ["02-01", "02-02", "02-03"]
files_modified:
  - execution/content_sheet.py
  - execution/virality_analyzer.py
  - execution/content_aggregate.py
  - directives/content_aggregate.md
  - output/content_sheet.csv
  - output/content_sheet.json
  - tests/test_content_sheet.py
  - tests/test_virality_analyzer.py
autonomous: true

must_haves:
  truths:
    - "Content sheet outputs CSV and JSON with full metadata"
    - "Virality analysis is structured/parseable (not prose summaries)"
    - "All sources integrated: Reddit, YouTube, Perplexity"
    - "Deduplication filters out content from last 4 weeks"
    - "DOE directive version updated to 2026.01.31"
  artifacts:
    - path: "execution/content_sheet.py"
      provides: "Content sheet generation"
      exports: ["generate_content_sheet", "save_csv", "save_json"]
      min_lines: 80
    - path: "execution/virality_analyzer.py"
      provides: "Structured virality analysis"
      exports: ["analyze_virality", "VIRALITY_SCHEMA"]
      min_lines: 100
    - path: "output/content_sheet.csv"
      provides: "CSV output of aggregated content"
    - path: "output/content_sheet.json"
      provides: "JSON output of aggregated content"
  key_links:
    - from: "execution/content_aggregate.py"
      to: "execution/youtube_fetcher.py"
      via: "import and orchestration"
      pattern: "from execution.youtube_fetcher import"
    - from: "execution/content_aggregate.py"
      to: "execution/perplexity_client.py"
      via: "import and orchestration"
      pattern: "from execution.perplexity_client import"
    - from: "execution/content_aggregate.py"
      to: "execution/deduplication.py"
      via: "duplicate filtering"
      pattern: "from execution.deduplication import"
    - from: "execution/content_sheet.py"
      to: "execution/virality_analyzer.py"
      via: "virality analysis integration"
      pattern: "from execution.virality_analyzer import"
---

<objective>
Create content sheet output module with virality analysis, integrate all sources (Reddit, YouTube, Perplexity) into updated content_aggregate.py, update DOE directive.

Purpose: Content sheet is the deliverable for Phase 2 - a structured output with all aggregated content, outlier scores, and AI-parseable virality analysis. This enables Phase 4 (Newsletter Engine) to work from structured data.

Output: Updated content_aggregate.py orchestrating all sources, content_sheet.py for CSV/JSON output, virality_analyzer.py for structured analysis.
</objective>

<execution_context>
@~/.config/Claude/get-shit-done/workflows/execute-plan.md
@~/.config/Claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/02-core-sources/02-CONTEXT.md
@.planning/phases/02-core-sources/02-RESEARCH.md
@.planning/phases/02-core-sources/02-01-SUMMARY.md
@.planning/phases/02-core-sources/02-02-SUMMARY.md
@.planning/phases/02-core-sources/02-03-SUMMARY.md
@execution/content_aggregate.py
@execution/youtube_fetcher.py
@execution/perplexity_client.py
@execution/deduplication.py
@directives/content_aggregate.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create virality analyzer module</name>
  <files>execution/virality_analyzer.py, tests/test_virality_analyzer.py</files>
  <action>
Create `execution/virality_analyzer.py` with structured virality analysis per CONTEXT.md decisions:

```python
"""
Virality analyzer for content.
DOE-VERSION: 2026.01.31

Produces structured, AI-parseable virality analysis (not prose summaries).
Primary consumer is the newsletter engine (Phase 4), not humans.

Per CONTEXT.md:
- "prove and define virality with a high degree of certainty"
- "uncover the ultimate legend or key to breaking norms"
- "scientific breakdown: given circumstances + variables â†’ why was virality almost inevitable?"
"""

from typing import Optional

# Schema for virality analysis output
VIRALITY_SCHEMA = {
    "hook_analysis": {
        "hook_type": str,  # question, statement, number, controversy, story
        "hook_text": str,  # The actual hook
        "attention_elements": list,  # What grabs attention
    },
    "emotional_triggers": [
        {
            "trigger": str,  # fear, greed, curiosity, urgency, fomo, hope
            "evidence": str,  # Specific text/element that triggers this
            "intensity": str,  # low, medium, high
        }
    ],
    "timing_factors": {
        "day_relevance": str,  # Why this timing worked
        "trending_topics": list,  # Related trending topics
        "seasonal_hook": str | None,  # Seasonal relevance if any
    },
    "success_factors": {
        "key_drivers": list,  # Primary reasons for virality
        "reproducible_elements": list,  # What can be replicated
        "unique_circumstances": list,  # One-time factors
    },
    "virality_confidence": str,  # definite, likely, possible, unclear
    "replication_notes": str,  # How to replicate this success
}


def analyze_virality(
    content: dict,
    transcript: str | None = None
) -> dict:
    """
    Analyze content for virality factors.
    
    Args:
        content: Content dict with title, description, views, etc.
        transcript: Optional transcript text for deeper analysis
    
    Returns:
        Structured virality analysis dict matching VIRALITY_SCHEMA
    """
    title = content.get("title", "")
    description = content.get("description", content.get("selftext", ""))
    
    analysis = {
        "hook_analysis": _analyze_hook(title),
        "emotional_triggers": _identify_triggers(title, description),
        "timing_factors": _analyze_timing(content),
        "success_factors": _identify_success_factors(content, transcript),
        "virality_confidence": _assess_confidence(content),
        "replication_notes": "",  # Filled after all analysis
    }
    
    # Generate replication notes from analysis
    analysis["replication_notes"] = _generate_replication_notes(analysis)
    
    return analysis


def _analyze_hook(title: str) -> dict:
    """Analyze the hook/title for attention-grabbing elements."""
    hook_types = {
        "question": ["?", "how", "why", "what", "when", "where"],
        "number": any(c.isdigit() for c in title),
        "controversy": ["unpopular", "wrong", "nobody", "truth"],
        "story": ["i ", "my ", "we ", "our "],
        "statement": True,  # Default
    }
    
    title_lower = title.lower()
    
    # Detect hook type (priority order)
    detected_type = "statement"
    if "?" in title:
        detected_type = "question"
    elif any(c.isdigit() for c in title):
        detected_type = "number"
    elif any(kw in title_lower for kw in hook_types["controversy"]):
        detected_type = "controversy"
    elif any(title_lower.startswith(kw) for kw in hook_types["story"]):
        detected_type = "story"
    
    # Identify attention elements
    attention_elements = []
    if "$" in title or any(kw in title_lower for kw in ["revenue", "profit", "income"]):
        attention_elements.append("money")
    if any(kw in title_lower for kw in ["secret", "hidden", "nobody knows"]):
        attention_elements.append("exclusivity")
    if any(kw in title_lower for kw in ["fast", "quick", "instant", "minutes"]):
        attention_elements.append("speed")
    if any(c.isdigit() for c in title):
        attention_elements.append("specificity")
    
    return {
        "hook_type": detected_type,
        "hook_text": title[:100],  # First 100 chars
        "attention_elements": attention_elements,
    }


def _identify_triggers(title: str, description: str) -> list[dict]:
    """Identify emotional triggers in content."""
    text = f"{title} {description}".lower()
    triggers = []
    
    trigger_patterns = {
        "fear": ["lose", "miss", "fail", "mistake", "wrong", "risk"],
        "greed": ["money", "profit", "revenue", "$", "income", "rich"],
        "curiosity": ["secret", "hidden", "discover", "reveal", "truth"],
        "urgency": ["now", "today", "limited", "last chance", "deadline"],
        "fomo": ["everyone", "trending", "viral", "popular", "others"],
        "hope": ["success", "achieve", "transform", "change", "better"],
    }
    
    for trigger, keywords in trigger_patterns.items():
        matches = [kw for kw in keywords if kw in text]
        if matches:
            triggers.append({
                "trigger": trigger,
                "evidence": f"Keywords: {', '.join(matches[:3])}",
                "intensity": "high" if len(matches) >= 3 else ("medium" if len(matches) >= 2 else "low"),
            })
    
    return triggers


def _analyze_timing(content: dict) -> dict:
    """Analyze timing factors."""
    return {
        "day_relevance": "standard",  # Would need date context
        "trending_topics": [],  # Would need external trend data
        "seasonal_hook": None,
    }


def _identify_success_factors(content: dict, transcript: str | None) -> dict:
    """Identify success factors."""
    outlier_score = content.get("outlier_score", 0)
    engagement_mods = content.get("engagement_modifiers", [])
    
    key_drivers = []
    reproducible = []
    unique = []
    
    if outlier_score >= 10:
        key_drivers.append("exceptional_performance_10x+")
    elif outlier_score >= 5:
        key_drivers.append("strong_performance_5x+")
    
    if engagement_mods:
        key_drivers.extend(engagement_mods)
        reproducible.append(f"engagement_hooks: {', '.join(engagement_mods)}")
    
    return {
        "key_drivers": key_drivers,
        "reproducible_elements": reproducible,
        "unique_circumstances": unique,
    }


def _assess_confidence(content: dict) -> str:
    """Assess confidence in virality assessment."""
    score = content.get("outlier_score", 0)
    
    if score >= 10:
        return "definite"
    elif score >= 5:
        return "likely"
    elif score >= 3:
        return "possible"
    else:
        return "unclear"


def _generate_replication_notes(analysis: dict) -> str:
    """Generate actionable replication notes from analysis."""
    notes = []
    
    hook = analysis.get("hook_analysis", {})
    if hook.get("hook_type"):
        notes.append(f"Hook pattern: {hook['hook_type']}")
    if hook.get("attention_elements"):
        notes.append(f"Use elements: {', '.join(hook['attention_elements'])}")
    
    triggers = analysis.get("emotional_triggers", [])
    high_triggers = [t["trigger"] for t in triggers if t.get("intensity") == "high"]
    if high_triggers:
        notes.append(f"Strong triggers: {', '.join(high_triggers)}")
    
    factors = analysis.get("success_factors", {})
    if factors.get("reproducible_elements"):
        notes.append(f"Replicate: {'; '.join(factors['reproducible_elements'])}")
    
    return " | ".join(notes) if notes else "Analyze manually"
```

Create `tests/test_virality_analyzer.py`:
- Test analyze_virality returns correct schema structure
- Test _analyze_hook detects different hook types
- Test _identify_triggers finds emotional triggers
- Test _assess_confidence returns correct levels
- Test _generate_replication_notes creates actionable notes
  </action>
  <verify>
```bash
python -c "from execution.virality_analyzer import analyze_virality, VIRALITY_SCHEMA; print('Imports OK')"
python -m pytest tests/test_virality_analyzer.py -v
```
  </verify>
  <done>
- virality_analyzer.py exists with structured analysis functions
- Schema is parseable and AI-friendly
- Tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Create content sheet module</name>
  <files>execution/content_sheet.py, tests/test_content_sheet.py</files>
  <action>
Create `execution/content_sheet.py` for CSV/JSON output:

```python
"""
Content sheet generator.
DOE-VERSION: 2026.01.31

Generates CSV and JSON content sheets with full metadata and virality analysis.
Per CONTEXT.md: Both formats generated each run.
"""

import csv
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

from execution.virality_analyzer import analyze_virality

OUTPUT_DIR = Path("output")

# CSV columns per OUTP-04
CSV_COLUMNS = [
    "source",
    "id",
    "title",
    "url",
    "thumbnail_url",
    "author",
    "published_at",
    "views",
    "engagement_score",  # upvotes for Reddit, views for YouTube
    "outlier_score",
    "hook_type",
    "emotional_triggers",
    "virality_confidence",
    "replication_notes",
]


def generate_content_sheet(
    contents: list[dict],
    include_virality: bool = True
) -> list[dict]:
    """
    Generate content sheet with virality analysis.
    
    Args:
        contents: List of content dicts from all sources
        include_virality: Whether to add virality analysis (default True)
    
    Returns:
        Enriched content list ready for output
    """
    enriched = []
    
    for content in contents:
        item = content.copy()
        
        if include_virality:
            virality = analyze_virality(content)
            item["virality_analysis"] = virality
            
            # Flatten key fields for CSV
            item["hook_type"] = virality.get("hook_analysis", {}).get("hook_type", "")
            triggers = virality.get("emotional_triggers", [])
            item["emotional_triggers"] = ", ".join(t["trigger"] for t in triggers)
            item["virality_confidence"] = virality.get("virality_confidence", "")
            item["replication_notes"] = virality.get("replication_notes", "")
        
        enriched.append(item)
    
    return enriched


def save_csv(
    contents: list[dict],
    filename: str = "content_sheet.csv",
    output_dir: Path | None = None
) -> Path:
    """Save content sheet as CSV."""
    output_dir = output_dir or OUTPUT_DIR
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filepath = output_dir / filename
    
    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=CSV_COLUMNS, extrasaction="ignore")
        writer.writeheader()
        
        for content in contents:
            # Map fields to CSV columns
            row = {
                "source": content.get("source", ""),
                "id": content.get("id", content.get("video_id", content.get("post_id", ""))),
                "title": content.get("title", ""),
                "url": content.get("url", ""),
                "thumbnail_url": content.get("thumbnail_url", content.get("thumbnail", "")),
                "author": content.get("author", content.get("channel_name", "")),
                "published_at": content.get("published_at", content.get("created_utc", "")),
                "views": content.get("views", content.get("upvotes", 0)),
                "engagement_score": content.get("upvotes", content.get("views", 0)),
                "outlier_score": f"{content.get('outlier_score', 0):.2f}",
                "hook_type": content.get("hook_type", ""),
                "emotional_triggers": content.get("emotional_triggers", ""),
                "virality_confidence": content.get("virality_confidence", ""),
                "replication_notes": content.get("replication_notes", ""),
            }
            writer.writerow(row)
    
    return filepath


def save_json(
    contents: list[dict],
    filename: str = "content_sheet.json",
    output_dir: Path | None = None
) -> Path:
    """Save content sheet as JSON with full metadata."""
    output_dir = output_dir or OUTPUT_DIR
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filepath = output_dir / filename
    
    output_data = {
        "metadata": {
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "total_items": len(contents),
            "sources": list(set(c.get("source", "unknown") for c in contents)),
        },
        "contents": contents,
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    return filepath


def generate_and_save(
    contents: list[dict],
    csv_filename: str = "content_sheet.csv",
    json_filename: str = "content_sheet.json",
    output_dir: Path | None = None
) -> tuple[Path, Path]:
    """Generate content sheet and save both CSV and JSON."""
    enriched = generate_content_sheet(contents)
    
    csv_path = save_csv(enriched, csv_filename, output_dir)
    json_path = save_json(enriched, json_filename, output_dir)
    
    return csv_path, json_path
```

Create `tests/test_content_sheet.py`:
- Test generate_content_sheet adds virality analysis
- Test save_csv creates valid CSV with headers
- Test save_json creates valid JSON with metadata
- Test generate_and_save produces both files
  </action>
  <verify>
```bash
python -c "from execution.content_sheet import generate_content_sheet, save_csv, save_json; print('Imports OK')"
python -m pytest tests/test_content_sheet.py -v
```
  </verify>
  <done>
- content_sheet.py exists with CSV and JSON output functions
- Virality analysis integrated
- Tests pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Update content_aggregate.py and DOE directive</name>
  <files>execution/content_aggregate.py, directives/content_aggregate.md</files>
  <action>
Update `execution/content_aggregate.py` to integrate all sources:

1. Update DOE-VERSION to 2026.01.31
2. Add imports for new modules:
   - youtube_fetcher
   - perplexity_client
   - transcript_fetcher
   - deduplication
   - content_sheet
3. Add new CLI arguments:
   - --sources: Comma-separated list (reddit,youtube,perplexity)
   - --no-youtube: Skip YouTube fetching
   - --no-perplexity: Skip Perplexity research
   - --no-dedup: Skip deduplication
   - --output-format: csv, json, or both (default: both)
4. Update run_aggregation function:
   - Fetch from all enabled sources
   - Apply deduplication
   - Generate content sheet
   - Save to output/ directory
5. Add new display format showing source counts

Update `directives/content_aggregate.md`:
1. Update DOE-VERSION to 2026.01.31
2. Add new CLI examples
3. Document all sources
4. Update output section

Key implementation:
```python
# In run_aggregation()
all_content = []

# Reddit (existing)
if "reddit" in sources:
    reddit_posts = fetch_all_subreddits(...)
    all_content.extend(reddit_posts)

# YouTube (new)
if "youtube" in sources:
    youtube_fetcher = YouTubeFetcher()
    videos = youtube_fetcher.fetch_all_channels(...)
    all_content.extend(videos)
    
    # Fetch transcripts for top 10
    top_videos = sorted(videos, key=lambda x: x.get("outlier_score", 0), reverse=True)[:10]
    transcripts = fetch_transcripts_batch([v["id"] for v in top_videos])
    save_transcripts(transcripts)

# Perplexity (new)
if "perplexity" in sources:
    trends = search_trends("DTC e-commerce")
    save_research(trends, "trends")
    # Add to content for reference

# Deduplication
if not skip_dedup:
    all_content, dup_count = filter_duplicates(all_content)
    print(f"Removed {dup_count} duplicates")

# Generate content sheet
csv_path, json_path = generate_and_save(all_content)
print(f"Content sheet saved to: {csv_path}, {json_path}")
```
  </action>
  <verify>
```bash
# Check updated imports work
python -c "from execution.content_aggregate import run_aggregation; print('OK')"

# Check DOE version updated
grep "DOE-VERSION: 2026.01.31" execution/content_aggregate.py
grep "DOE-VERSION: 2026.01.31" directives/content_aggregate.md

# Check CLI help shows new options
python execution/content_aggregate.py --help
```
  </verify>
  <done>
- content_aggregate.py updated with all source integrations
- DOE versions match (2026.01.31)
- New CLI arguments work
- Content sheet generated on each run
  </done>
</task>

</tasks>

<verification>
1. All new modules import correctly
2. `python execution/content_aggregate.py --help` shows all new options
3. DOE versions match between directive and script (2026.01.31)
4. Content sheet files generated in output/ directory
5. All tests pass: `python -m pytest tests/test_virality_analyzer.py tests/test_content_sheet.py -v`
</verification>

<success_criteria>
- Virality analyzer produces structured, AI-parseable output
- Content sheet outputs both CSV and JSON with full metadata
- All sources integrated: Reddit, YouTube, Perplexity
- Deduplication filters content from last 4 weeks
- DOE directive and script versions match
- Phase 2 requirements complete: AGGR-01, AGGR-03, AGGR-07, AGGR-08, AGGR-09, OUTP-04
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-sources/02-04-SUMMARY.md`
</output>
